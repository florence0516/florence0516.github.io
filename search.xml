<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Bert模型是否适用于文本相似度计算任务</title>
    <url>/2020/12/06/Bert%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E9%80%82%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<p>随着2018年底Bert的面世，NLP进入了预训练模型的时代。各大预训练模型如GPT-2，Robert，XLNet，Transformer-XL，Albert，T5等等层数不穷，但是几乎大部分的预训练模型都不适合非监督任务。</p>
<a id="more"></a>

<h3 id="Bert模型是否适用于文本相似度计算任务"><a href="#Bert模型是否适用于文本相似度计算任务" class="headerlink" title="Bert模型是否适用于文本相似度计算任务"></a>Bert模型是否适用于文本相似度计算任务</h3><p>先说结论：我的这个文本相似度计算任务是通过网络爬虫获取的数据，是没有相似度标签的（比如0/1表示不相似/相似），是无监督训练。对于这种情况，不能够通过fine-tune（指的是用别人训练好的模型，即预训练模型，加上我们自己的数据，来训练新的模型）进行微调，用Bert提取句向量再通过余弦函数计算相似度的效果并不好甚至不如用词嵌入模型Glove的效果好。</p>
<p>本文主要参考了2019年8月底发表在EMNLP会议上的一篇论文：</p>
<blockquote>
<p>Reimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks[J]. arXiv preprint arXiv:1908.10084, 2019.</p>
</blockquote>
<p>论文中指出随着2018年底Bert的面世，NLP进入了预训练模型的时代。各大预训练模型如GPT-2，Robert，XLNet，Transformer-XL，Albert，T5等等层数不穷，但是几乎大部分的预训练模型均不适合语义相似度搜索，也不适合非监督任务。</p>
<p>解决聚类和语义相似度计算的一种常见方法是将每个句子映射到一个向量空间，使得语义相似的句子很接近。有人会尝试将整个句子输入预训练模型中，得到该句的句向量，然后作为句子的句向量表示，但是在这篇论文中指出这样得到的句向量表达语义信息的能力并不好，也就是说，两个相似的句子，得到的句向量可能会有很大的差别。</p>
<p>文中还列出了对比实验结果，这里我们主要关注<strong>无监督</strong>训练结果：</p>
<h6 id="无监督训练"><a href="#无监督训练" class="headerlink" title="无监督训练"></a>无监督训练</h6><ul>
<li><p>文中的评测采用的是STS 2012-2016五年的任务数据、STS benchmark数据(2017年构建)、SICK-Relatedness数据，这些数据集都是标好label的句子对，label表示句子之间的相互关系，范围为0~5（0是没什么关系，5表示句子非常相近）。</p>
</li>
<li><p>无监督评测不采用这些数据集的任何训练数据，直接用训练好的模型得到句向量+余弦函数来计算句子间的相似度，然后通过斯皮尔曼等级相关系数来衡量模型的优劣。实验结果如下：</p>
<p><img src="/2020/12/06/Bert%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E9%80%82%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E4%BB%BB%E5%8A%A1/%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%9C.png" alt="论文结果"></p>
<p>结果显示直接采用BERT的输出结果，效果并不好，甚至不如GloVe嵌入向量的效果好；后两栏表示采用本文的孪生网络在NLI数据集上fine-tune后的模型效果，会明显要好很多，但是SBERT和SRoBERTa差异不大。</p>
<p><strong>注意：</strong></p>
<ul>
<li>CLS和avg是本文尝试用Bert模型得到句向量的不同方法：CLS是以第一个特征位置向量作为句向量；avg是取输出层的所有字向量的平均值作为句向量。</li>
<li>InferSent是一种句子embeddings方法，它为英语句子提供语理解。</li>
<li>Universal Sentence Encoder：通用句子编码器。</li>
<li>base/large：是Bert模型的两种类型，一个是12层Transformer，一个是24层Transformer。</li>
</ul>
</li>
</ul>
<h6 id="有监督训练"><a href="#有监督训练" class="headerlink" title="有监督训练"></a>有监督训练</h6><ul>
<li><p>但另一方面，如果是有监督训练，经过fine-tune，预训练模型的效果都有很大的提升，其中Bert达到了最好的效果：</p>
<p><img src="/2020/12/06/Bert%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%90%A6%E9%80%82%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97%E4%BB%BB%E5%8A%A1/%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%9C2.png" alt="论文结果2"></p>
<ul>
<li>有监督STS数据集采用的是STS benchmark（简称STSb）数据集，就是上面提到的2017年抽取构建的，是当前比较流行的有监督STS数据集。它主要来自三个方面：字幕、新闻、论坛，包含8,628个句子对，训练集5,749，验证集1,500，测试集1,379。BERT将句子对同时输入网络，最后再接一个简单的回归模型作为输出，目前在此数据集上取得了最好的效果。</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>文本相似度计算</tag>
      </tags>
  </entry>
  <entry>
    <title>命名实体识别（NER）调研</title>
    <url>/2020/11/27/NER/</url>
    <content><![CDATA[<p>本文介绍了现有的一些关于命名实体识别技术的研究方法，命名实体识别技术是信息抽取技术的一部分，是很多任务的基础工作。</p>
<a id="more"></a>

<h3 id="命名实体识别技术调研"><a href="#命名实体识别技术调研" class="headerlink" title="命名实体识别技术调研"></a>命名实体识别技术调研</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>命名实体就是命名实体识别的研究主体，一般包括三大类（实体类、时间类和数字类）和七小类（人名、地名、机构名、时间、日期、货币和百分比）命名实体。</p>
<p>命名实体识别（Named Entity Recognition，NER）又称专名识别，从早期基于字典和规则的方法，到传统机器学习的方法，再到近年来基于深度学习的各类方法，其研究进展的大概趋势如下图所示。</p>
<img src="/2020/11/27/NER/p1.png" alt="NER发展趋势" style="zoom:80%;">

<h4 id="基于字典和规则的方法"><a href="#基于字典和规则的方法" class="headerlink" title="基于字典和规则的方法"></a>基于字典和规则的方法</h4><p>基于字典和规则的方法是命名实体识别任务中最古老的方法。利用手工编写的规则，提取特征，比如关键词、指示词、位置词等，收集特征词，并且给每一个规则都赋予一个权值，当规则冲突的时候，选择权值最高的规则进行命名实体类型的判别；但是这些规则往往依赖于具体语言、领域和文本风格，编制过程耗时且难以涵盖所有的语言现象，特别容易产生错误，系统可移植性不好，对于不同的系统需要语言学专家重新书写规则。</p>
<h4 id="基于传统机器学习的方法"><a href="#基于传统机器学习的方法" class="headerlink" title="基于传统机器学习的方法"></a>基于传统机器学习的方法</h4><p>基于传统机器学习的方法主要包括隐马尔可夫模型（Hidden Markov Model，HMM）、支持向量机（support vector machines，SVM）、条件随机场[1]（Conditional Random Fields，CRF）等。在此类方法中，NER被当作序列标注问题，利用大规模语料来学习出标注模型，从而对句子的各个位置进行标注。隐马尔可夫模型（HMM）认为观测到的句子中的每个词都是相互独立的，而且当前时刻的标注只与前一时刻的标注相关，但实际上，句子中的每个词是存在依赖关系的，且当前时刻的标注应该与前一时刻以及后一时刻的标注都相关联，因此，HMM模型虽然在训练以及识别时的效率更高，但是准确率比较低，效果不够理想；条件随机场（CRF）为命名实体识别提供了一个特征灵活、全局最优的标注框架，能对隐含状态建模，学习状态序列的特点，在标注准确率上有一定优势，但是收敛速度较慢、所需的训练时间也更长。基于统计机器学习的方法中，CRF是使用最广泛的，它的目标函数不仅考虑输入的状态特征函数，而且还包含了标签转移特征函数。在训练时可以使用随机梯度下降法（Stochastic Gradient Descent，SGD）学习模型参数。在已知模型时，给输入序列求预测输出序列即求使目标函数最大化的最优序列，是一个动态规划问题，可以使用Viterbi算法解码来得到最优标签序列。</p>
<h4 id="基于深度学习的方法"><a href="#基于深度学习的方法" class="headerlink" title="基于深度学习的方法"></a>基于深度学习的方法</h4><p>随着硬件计算能力的发展以及词的分布式表示（word embedding）的提出，神经网络可以有效处理这种序列标注任务：将分词从离散的独热向量（one-hot）表示映射到低维空间中成为稠密的词向量（embedding），随后将句子的embedding序列输入到卷积神经网络（Convolutional Neural Networks，CNN）或循环神经网络（Recurrent Neural Network，RNN）中，用神经网络自动提取特征，归一化指数（Softmax）函数来预测每个分词的标签。这种方法使得模型的训练成为一个端到端的过程，不依赖于特征工程，是一种数据驱动的方法，但网络种类繁多、对参数设置依赖大，模型可解释性差。此外，这种方法的一个缺点是对每个分词打标签的过程是独立的进行，不能直接利用上文已经预测的标签，进而导致预测出的标签序列可能是无效的。因此，学界提出了深度学习模型（DL）+CRF的混合模型做序列标注，在神经网络的输出层接入CRF层来做句子级别的标签预测，使得标注过程不再是对各个分词的独立分类。</p>
<h4 id="近期工作"><a href="#近期工作" class="headerlink" title="近期工作"></a>近期工作</h4><p>近年来，在基于神经网络结构的NER研究主要集中在三个方面：一是，使用流行的注意力机制[2]（Attention Mechanism）来提高模型效果；二是，针对少量标注训练数据或无标注数据进行的一些研究，其中包括了迁移学习[3]和半监督学习[4]，这也是未来研究的重点。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1].  Lafferty J, McCallum A, Pereira F C N. Conditional random fields: Probabilistic models for segmenting and labeling sequence data[J]. 2001.</p>
<p>[2].  Zukov-Gregoric A, Bachrach Y, Minkovsky P, et al. Neural named entity recognition using a self-attention mechanism[C]. 2017 IEEE 29th International Conference on Tools with Artificial Intelligence (ICTAI). IEEE, 2017: 652-656.</p>
<p>[3].  Beryozkin G, Drori Y, Gilon O, et al. A Joint Named-Entity Recognizer for Heterogeneous Tag-sets Using a Tag Hierarchy[J]. arXiv preprint arXiv:1905.09135, 2019.</p>
<p>[4].  Peters M E, Ammar W, Bhagavatula C, et al. Semi-supervised sequence tagging with bidirectional language models[J]. arXiv preprint arXiv:1705.00108, 2017.</p>
]]></content>
      <tags>
        <tag>调研报告</tag>
      </tags>
  </entry>
  <entry>
    <title>命名实体识别任务中HMM、CRF、Bi-LSTM+CRF模型详解</title>
    <url>/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>本文主要从宏观意义上解释HMM、CRF和Bi-LSTM+CRF模型的含义，并没有涉及到复杂的公式推导。如果说了解一个算法分为两步：会其意、知其形，那我们这篇文章主要是在说第一点，但做到这一点，恰恰是对于我们真正理解并会用一个模型最重要的。</p>
<a id="more"></a>

<h3 id="一、命名实体识别技术中的HMM"><a href="#一、命名实体识别技术中的HMM" class="headerlink" title="一、命名实体识别技术中的HMM"></a>一、命名实体识别技术中的HMM</h3><p><strong>隐马尔可夫模型</strong>（Hidden Markov Model，HMM）是统计模型，它用来描述从可观察的参数中确定该过程的隐含参数。在命名实体识别（NER）技术中，就是求观察序列（文本）的背后最可能的实体类别标注序列。</p>
<h4 id="1-例子引入"><a href="#1-例子引入" class="headerlink" title="1. 例子引入"></a>1. 例子引入</h4><p>假设我手里有三个不同的骰子。第一个骰子是我们平常见的骰子（称这个骰子为D6），6个面，每个面（1，2，3，4，5，6）出现的概率是1/6。第二个骰子是个四面体（称这个骰子为D4），每个面（1，2，3，4）出现的概率是1/4。第三个骰子有八个面（称这个骰子为D8），每个面（1，2，3，4，5，6，7，8）出现的概率是1/8。</p>
<p><img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/%E9%AA%B0%E5%AD%90.png" alt="骰子示例"></p>
<p>假设我们开始掷骰子，我们先从三个骰子里挑一个，挑到每一个骰子的概率都是1/3。然后我们掷骰子，得到一个数字，1，2，3，4，5，6，7，8中的一个。不停的重复上述过程，我们会得到一串数字，每个数字都是1，2，3，4，5，6，7，8中的一个。例如我们可能得到这么一串数字（掷骰子10次）：1 6 3 5 2 7 3 5 2 4。这串数字叫做可见状态链。但是在隐马尔可夫模型中，我们不仅仅有这么一串可见状态链，还有一串隐含状态链。在这个例子里，这串隐含状态链就是你用的骰子的序列。比如，隐含状态链有可能是：D6 D8 D8 D6 D4 D8 D6 D6 D4 D8。</p>
<p>一般来说，HMM中说到的马尔可夫链其实是指隐含状态链，因为隐含状态（骰子）之间存在转换概率（transition probability）。在我们这个例子里，D6的下一个状态是D4，D6，D8的概率都是1/3。D4，D8的下一个状态是D4，D6，D8的转换概率也都一样是1/3。这样设定是为了最开始容易说清楚，但是我们其实是可以随意设定转换概率的。比如，我们可以这样定义，D6后面不能接D4，D6后面是D6的概率是0.9，是D8的概率是0.1。这样就是一个新的HMM。</p>
<p>同样的，尽管可见状态之间没有转换概率，但是隐含状态和可见状态之间有一个概率叫做输出概率（emission probability）。就我们的例子来说，六面骰（D6）产生1的输出概率是1/6。产生2，3，4，5，6的概率也都是1/6。我们同样可以对输出概率进行其他定义。比如，我有一个被赌场动过手脚的六面骰子，掷出来是1的概率更大，是1/2，掷出来是2，3，4，5，6的概率是1/10。</p>
<p><img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="隐马尔可夫模型示意图"></p>
<p><img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/%E9%9A%90%E5%90%AB%E7%8A%B6%E6%80%81%E8%BD%AC%E6%8D%A2%E5%85%B3%E7%B3%BB%E7%A4%BA%E6%84%8F%E5%9B%BE.png" alt="隐含状态转换关系示意图"></p>
<p>其实对于HMM来说，如果提前知道所有隐含状态之间的转换概率和所有隐含状态到所有可见状态之间的输出概率，做模拟是相当容易的。但是应用HMM模型时候呢，往往是缺失了一部分信息的，有时候你知道骰子有几种，每种骰子是什么，但是不知道掷出来的骰子序列；有时候你只是看到了很多次掷骰子的结果，剩下的什么都不知道。如果应用算法去估计这些缺失的信息，就成了一个很重要的问题。</p>
<h4 id="2-NER中的HMM"><a href="#2-NER中的HMM" class="headerlink" title="2. NER中的HMM"></a>2. NER中的HMM</h4><p>由上述例子可以看出隐马尔科夫模型包含5个基本元素：隐含状态的有限集合、观察值的有限集合、隐含状态的转移概率矩阵、隐含状态输出观测值的输出概率矩阵以及一个初始概率矩阵。</p>
<p>用隐马尔科夫模型做命名实体识别（NER），其实就是根据输入的一系列单词，去生成其背后的标注，从而得到实体。下表是结合NER任务对隐马尔科夫模型的5个基本元素做的说明：</p>
<table>
<thead>
<tr>
<th align="center">HMM基本元素</th>
<th align="center">NER任务中的具体解释</th>
</tr>
</thead>
<tbody><tr>
<td align="center">隐含状态的有限集合</td>
<td align="center">每一个词语背后的标注</td>
</tr>
<tr>
<td align="center">观察值的有限集合</td>
<td align="center">每一个词语本身</td>
</tr>
<tr>
<td align="center">隐含状态转移概率矩阵</td>
<td align="center">某一个标注转移到下一个标注的概率</td>
</tr>
<tr>
<td align="center">输出概率矩阵</td>
<td align="center">在某个标注下，生成某个词的概率</td>
</tr>
<tr>
<td align="center">初始概率矩阵</td>
<td align="center">每一个标注的初始化概率</td>
</tr>
</tbody></table>
<p>以上的这些元素都可以从现有的训练语料集中统计出来。最后，根据这些统计值，应用维特比（viterbi）算法（就是求解HMM上的最大概率的算法），就可以算出词语序列背后的标注序列，从而从标注序列中提取出实体块。</p>
<p><img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/HMM.png" alt="HMM"></p>
<p>上图是一个简单的隐马尔科夫模型示例，X序列就是就是NER任务中的文本序列，yi就代表了每个隐含状态，即NER任务中的实体类别标注序列。</p>
<p>可以看出，隐马尔科夫模型存在两个假设：一是输出观察值之间严格独立，二是状态转移过程中当前状态只与前一状态有关。也就是说，在命名实体识别的场景下，隐马尔科夫模型认为观测到的句子中的每个词都是相互独立的，而且当前时刻的标注只与前一时刻的标注相关。</p>
<p>但实际上，命名实体识别往往需要更多的特征，比如词性，词的上下文等等，而且当前时刻的标注应该与前一时刻以及后一时刻的标注都相关联。因此由于这两个假设的存在，显然HMM模型在解决命名实体识别的问题上是存在缺陷的。</p>
<h3 id="二、命名实体识别技术中的CRF"><a href="#二、命名实体识别技术中的CRF" class="headerlink" title="二、命名实体识别技术中的CRF"></a>二、命名实体识别技术中的CRF</h3><p><img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/CRF.png" alt="CRF"></p>
<p>上图给出简单的条件随机场（CRF）模型示例，可以看出，条件随机场是一种无向图模型。</p>
<p>条件随机场CRF针对HMM模型中存在的不足，引入<strong>自定义</strong>的特征函数，不仅可以表达观测之间的依赖，还可表示当前观测与前后多个状态之间的复杂依赖，可以有效克服HMM模型面临的问题。</p>
<p>一个特征函数可以表示为 <img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/%E7%89%B9%E5%BE%81%E5%87%BD%E6%95%B0.png" alt="特征函数"> ，其中 X 表示输入序列， i 为当前的位置，yi 是当前隐含状态，y_{i-1} 表示前一个隐含状态，此时这个特征函数的表示含义与HMM中的转移概率含义是一致的，但事实上它可以有更丰富的表达。</p>
<p>特征方程的存在允许CRF有十分自由的特征表达， <img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/%E5%85%AC%E5%BC%8F1.png" alt="公式1"> 也可以是 <img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/%E5%85%AC%E5%BC%8F2.png" alt="公式2">。这样特征函数就可以表示当前的隐含状态与任意一个观测状态或者任意一个隐含状态甚至未来的某个隐含状态的关系。</p>
<p>但上述两个方法（HMM、CRF）是基于统计学习的方法，需要人为地从数据集中经过统计得到特征规律（转移概率或输出概率），人工工作量较大、速度比较慢。</p>
<h3 id="三、命名实体识别技术中的Bi-LSTM-CRF混合模型"><a href="#三、命名实体识别技术中的Bi-LSTM-CRF混合模型" class="headerlink" title="三、命名实体识别技术中的Bi-LSTM+CRF混合模型"></a>三、命名实体识别技术中的Bi-LSTM+CRF混合模型</h3><p>基于深度学习的方法可以将原本如CRF等模型与深度学习神经网络结合起来，其速度足够快，准确率也高，且不需要人为设定特征，能够从原始数据中自主的学习，找到更深层次和更加抽象的特征，是目前解决命名实体识别任务的主流方法，其中最常用的就是BiLSTM+CRF的混合模型。</p>
<p>该模型分为两层：一层Bi-LSTM层、一层CRF层。下图是它的结构图。</p>
<p><img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/BILSTM+CRF.png" alt="BILSTM+CRF结构图"></p>
<h4 id="1-Bi-LSTM神经网络模型"><a href="#1-Bi-LSTM神经网络模型" class="headerlink" title="1. Bi-LSTM神经网络模型"></a>1. Bi-LSTM神经网络模型</h4><p><img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/Bi-LSTM.png" alt="Bi-LSTM"></p>
<ul>
<li><p>目的：</p>
<p>通过一个前向LSTM网络和一个后向LSTM网络分别从前和后两个方向对句子进行建模，同时考虑了词语的前后文信息，能够更好的捕捉观测序列之间的依赖关系，使其在命名实体识别任务中可以取得更好的效果。</p>
</li>
<li><p>输入：句子中每个单词的词向量；</p>
</li>
<li><p>输出：每个隐含状态的输出分数，也就是该单词对应各个实体类别的分数（分数越高表示此类别的概率越大）。如w0节点的输出表示：是“B-Person”的分数为1.5，是“I-Person”的分数为0.9，是“B-Organization”的分数为0.1，是“I-Organization”的分数为0.08，是“O”的分数为0.05。</p>
</li>
</ul>
<h4 id="2-CRF层"><a href="#2-CRF层" class="headerlink" title="2. CRF层"></a>2. CRF层</h4><p><img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/Bi-lstm+crf.png" alt="Bi-lstm+crf"></p>
<ul>
<li><p>输入：Bi-LSTM层输出的隐含状态输出分数；</p>
</li>
<li><p>输出：所有可能的标注序列的得分。</p>
</li>
<li><p>目的：</p>
<p>在训练的过程中，学习隐含状态（实体类别）之间的转移分数矩阵。换句话说，就是对最终的预测结果添加一些限制来确保结果是有效的。转移分数矩阵示例如下：</p>
<p><img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/%E8%BD%AC%E7%A7%BB%E5%88%86%E6%95%B0%E7%9F%A9%E9%98%B5.png" alt="转移分数矩阵"></p>
<p>我们可以看到转移分数矩阵可以学到一些有用的限制条件：</p>
<ul>
<li>句子的开始应该是 “B-“ 或 “O”,而不是 “I-“（从START到I-Person或I-Organization的得分非常低）</li>
<li>“B-label1 I-label2 I-label3 I-…”, 在这个模式中, label1, label2, label3 应该是同一个实体label.例如 “B-Person I-Person”是可以的（例如 “B-Organization”到“I-Person” 仅仅是0.0003）</li>
<li>“O I-label” 不合法（t_{O,I-PERSON}的得分非常低）</li>
</ul>
<p>那么如何得到转移分数矩阵呢？其实它是Bi-LSTM+CRF模型的一个参数。在开始训练模型之前，可以随机初始化，所有随机初始化的分数会在训练过程中自动更新。</p>
</li>
<li><p>训练过程中的损失函数：</p>
<p><img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B01.png" alt="损失函数1"></p>
</li>
</ul>
<p>  上式是由输入序列X得到真实标记序列y的概率的式子，其中，s(X, y)表示由输入序列 X 预测出真实标注序列 y 的分数。式子的含义是真实标注序列在所有可能标注序列中的分数占比。</p>
<ul>
<li><p>可见，我们训练的目的是使真实标注序列在所有可能标注序列中有最高的分数；</p>
</li>
<li><p>也就是，在训练过程中，不断更新 Bi-LSTM+CRF 模型的参数，使得真实标注序列的比重保持不断增加；</p>
<p>上式取对数得到损失函数：</p>
<p><img src="/2020/12/06/NER%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84HMM%E3%80%81CRF%E5%92%8CBi-LSTM-CRF%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B02.png" alt="损失函数2"></p>
</li>
<li><p>因为通常是最小化损失，所以我们也可以对上式取负然后最小化它，这样我们就可以使用梯度下降等优化方法来求解参数，也就训练了转移分数矩阵和Bi-LSTM中的参数，从而能够得到最可能的实体类别标注序列，判断实体位置。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows下Github+Hexo搭建个人网站</title>
    <url>/2020/11/22/Windows%E4%B8%8BGithub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/</url>
    <content><![CDATA[<p>Hexo是一款简约的静态博客框架，无广告、依赖少、易于安装使用、方便管理维护，可以生成静态网页并将其托管到Github上，对中文的支持也非常友好。我整理了自己的搭建过程，做了个简单的教程~</p>
<a id="more"></a>

<h2 id="Windows下Github-Hexo搭建个人网站"><a href="#Windows下Github-Hexo搭建个人网站" class="headerlink" title="Windows下Github + Hexo搭建个人网站"></a>Windows下Github + Hexo搭建个人网站</h2><h4 id="搭建步骤"><a href="#搭建步骤" class="headerlink" title="搭建步骤"></a>搭建步骤</h4><ol>
<li>安装Git</li>
<li>安装Node.js</li>
<li>配置Github pages</li>
<li>安装Hexo</li>
<li>推送网站</li>
<li>Hexo使用</li>
</ol>
<hr>
<h4 id="安装Git"><a href="#安装Git" class="headerlink" title="安装Git"></a>安装Git</h4><ul>
<li><p>Git是开源的分布式版本控制系统，用于敏捷高效地处理项目。个人网站在本地搭建好了，需要使用Git同步到GitHub上。附上<a href="https://git-scm.com/download/win">Git的官网下载地址</a>。Git的安装教程网上有很多，可自行查阅，不过多说明。</p>
</li>
<li><p>安装成功后，在开始菜单里搜索Git Bash，设置user.name和user.email配置信息：</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;你的GitHub用户名&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;你的GitHub注册邮箱&quot;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>输入命令：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/.ssh</span><br></pre></td></tr></table></figure>

<p>如果显示<code>No such file or directory</code>，则表示此电脑第一次使用SSH key。</p>
<ul>
<li>下面说明如何生成ssh密钥文件，输入命令：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;你的GitHub注册邮箱&quot;</span><br></pre></td></tr></table></figure>

<p>直接三个回车即可，默认不需要设置密码。</p>
<ul>
<li>然后找到生成的.ssh的文件夹中的id_rsa.pub密钥，将内容全部复制，</li>
</ul>
<p><img src="/2020/11/22/Windows%E4%B8%8BGithub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/p1.png" alt="image1"></p>
<p>打开<a href="https://github.com/settings/keys">Github_Settings_keys</a>页面，新建new SSH Key，</p>
<p><img src="/2020/11/22/Windows%E4%B8%8BGithub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/p2.png" alt="image2"></p>
<p>Title为标题，任意填即可，将刚刚复制的id_rsa.pub内容粘贴到Key字段，最后点击Add SSH key。</p>
<ul>
<li>在Git Bash中检测GitHub公钥设置是否成功，输入：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh git@github.com</span><br></pre></td></tr></table></figure>

<p>出现Connection to github.com closed，说明成功。这里之所以设置GitHub密钥原因是，通过非对称加密的公钥与私钥来完成加密，公钥放置在GitHub上，私钥放置在自己的电脑里。GitHub要求每次推送代码都是合法用户，所以每次推送都需要输入账号密码验证推送用户是否是合法用户，为了省去每次输入密码的步骤，采用了ssh，当你推送的时候，git就会匹配你的私钥跟GitHub上面的公钥是否是配对的，若是匹配就认为你是合法用户，则允许推送。这样可以保证每次的推送都是正确合法的。</p>
<hr>
<h4 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h4><ul>
<li>Hexo基于Node.js，附上<a href="https://nodejs.org/en/download/">Node.js下载地址</a>，下载安装包（LTS版本即可），注意安装Node.js会包含环境变量及npm的安装，安装后，检测Node.js是否安装成功，在命令行中输入：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">node -v</span><br></pre></td></tr></table></figure>

<ul>
<li>检测npm是否安装成功，在命令行中输入:</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm -v</span><br></pre></td></tr></table></figure>

<p>到这儿，安装Hexo的环境已经全部搭建完成。</p>
<hr>
<h4 id="配置Github-pages"><a href="#配置Github-pages" class="headerlink" title="配置Github pages"></a>配置Github pages</h4><ul>
<li>登录Github，点击”New repository”，新建一个仓库。输入仓库名：你的Github名称.github.io，然后点击 Create repository 。</li>
</ul>
<p><img src="/2020/11/22/Windows%E4%B8%8BGithub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/p7.png" alt="image3"></p>
<p>注意：一定要用自己的github的用户名，不然显示不出来。用户名尽量选择小写字母+数字的组合。由于我已经创建过主页了，因此会显示已经存在，正常应该是一个绿色的对号。</p>
<ul>
<li>然后就是启用 GitHub Pages。点击仓库右边的 Setting 菜单进入设置，下滑找到GitHub Pages，选择分支是master，然后点击 Choose a theme随意选择一个模版，点击 Select theme ，发布github默认生成的一个静态站点。</li>
</ul>
<p><img src="/2020/11/22/Windows%E4%B8%8BGithub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/p8.png" alt="image4"></p>
<ul>
<li>最后，在Setting的左侧导航栏找到“Branches”，在Default branch中设置为master。</li>
</ul>
<p><img src="/2020/11/22/Windows%E4%B8%8BGithub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/p9.png" alt="image5"></p>
<hr>
<h4 id="安装Hexo"><a href="#安装Hexo" class="headerlink" title="安装Hexo"></a>安装Hexo</h4><ul>
<li>Hexo就是我们的个人博客网站的框架， 这里需要自己在电脑常里创建一个文件夹，可以命名为blog，Hexo框架与以后你自己发布的网页都在这个文件夹中。创建好后，进入文件夹中，右键，点击“Git Bash Here”：</li>
</ul>
<p><img src="/2020/11/22/Windows%E4%B8%8BGithub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/p3.png" alt="image6"></p>
<ul>
<li>使用npm命令安装Hexo，输入：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install -g hexo</span><br></pre></td></tr></table></figure>

<ul>
<li>安装完成后，初始化我们的博客，输入：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo init hexo</span><br></pre></td></tr></table></figure>

<p>注意，这里的命令都是作用在刚刚创建的blog文件夹中。</p>
<ul>
<li>为了检测我们的网站雏形，进入到hexo目录，分别按顺序输入以下三条命令：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new test_my_site		<span class="comment"># 新建文章</span></span><br><span class="line">hexo g						<span class="comment"># 生成网页</span></span><br><span class="line">hexo s						<span class="comment"># 启动本地预览</span></span><br></pre></td></tr></table></figure>

<ul>
<li>完成后，打开浏览器输入地址：<a href="http://loalhost:4000/">localhost:4000</a>，可以看出我们写出第一篇博客：</li>
</ul>
<p><img src="/2020/11/22/Windows%E4%B8%8BGithub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/p4.png" alt="image7"></p>
<p>恭喜！出现此页面说明安装成功，在Git Bash中按<code>Ctrl+C</code>退出预览。</p>
<hr>
<h4 id="推送网站"><a href="#推送网站" class="headerlink" title="推送网站"></a>推送网站</h4><ul>
<li>上面只是在本地预览，接下来要做的就是就是推送网站，也就是发布网站，让我们的网站可以被更多的人访问。接下来需要将我们的Hexo与GitHub关联起来，打开hexo文件夹下的_config.yml文件，如下图：</li>
</ul>
<p><img src="/2020/11/22/Windows%E4%B8%8BGithub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/p5.png" alt="image8"></p>
<ul>
<li>翻到最后修改为：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line"><span class="built_in">type</span>: git</span><br><span class="line">repo: 这里填入你之前在GitHub上创建仓库的完整路径，记得加上 .git</span><br><span class="line">branch: master</span><br></pre></td></tr></table></figure>

<p>参考如下：</p>
<p><img src="/2020/11/22/Windows%E4%B8%8BGithub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/p6.png" alt="image9"></p>
<p>注意：’:’后一定留有空格，不然后续会报错。</p>
<ul>
<li>完成配置后保存文件。其实这就是给<code>hexo d</code>这个命令做相应的配置，让hexo知道你要把blog部署在哪个位置，很显然，我们部署在我们GitHub的仓库里。最后安装Git部署插件，输入命令：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>

<ul>
<li>这时，我们分别输入三条命令：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo clean			<span class="comment"># 清除缓存</span></span><br><span class="line">hexo g				<span class="comment"># 生成网页</span></span><br><span class="line">hexo d				<span class="comment"># 部署</span></span><br></pre></td></tr></table></figure>

<p>其实，第三条的<code>hexo d</code>就是部署网站命令，d是deploy的缩写。</p>
<ul>
<li>完成后，打开浏览器，在地址栏输入你的放置个人网站的仓库路径，即<a href="https://xxxx.github.io,这里的xxxx就是你的github用户名./">https://xxxx.github.io，这里的xxxx就是你的GitHub用户名。</a></li>
<li>你就会发现你的博客已经上线了，可以在网络上被访问了。P.S. 如果访问不了，尝试换DNS域名解析服务器，我的设置如下：</li>
</ul>
<p><img src="/2020/11/22/Windows%E4%B8%8BGithub-Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99/p10.png" alt="image10"></p>
<p>至此就大功告成啦！</p>
]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>短文本相似度计算调研</title>
    <url>/2020/11/27/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E5%BA%A6/</url>
    <content><![CDATA[<p>本文是对短文本相似度计算任务的调研报告，简单介绍了目前解决该问题的一些方法。</p>
<a id="more"></a>

<h3 id="短文本相似度计算调研"><a href="#短文本相似度计算调研" class="headerlink" title="短文本相似度计算调研"></a>短文本相似度计算调研</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>短文本相似度计算，即求解两个短文本之间的相似程度。目前，短文本相似度算法可以分为三大类：无监督相似度计算、有监督相似度计算，以及有监督+无监督的相似度计算。</p>
<h4 id="无监督相似度计算"><a href="#无监督相似度计算" class="headerlink" title="无监督相似度计算"></a>无监督相似度计算</h4><p>无监督相似度计算，又分为基于词汇重合度的TF-IDF（Term Frequency–Inverse Document Frequency）、BM25、Jaccard、VSM（Vector Space Model）、SimHash等模型，基于浅层语义的主题模型：LDA（Latent Dirichlet Allocation）、PLSA[1]（Probabilistic Latent Semantic Analysis）等，基于语义的编码模型：word to vector、WMD[2]（Word Mover’s Distance）、预训练编码器（如XLNet[3]、BERT[4]、ELMo[5]）等，以及基于以上各种方法的相似度分数的经验加权叠加。无监督方法的好处是不需要训练数据，可以直接通用于任何场景，缺点是准确度一般，常用于冷启动的场景。</p>
<h4 id="有监督相似度计算"><a href="#有监督相似度计算" class="headerlink" title="有监督相似度计算"></a>有监督相似度计算</h4><p>有监督的方法可以分为基于传统的分类模型，如LR（Logistic regression）、SVM、GBDT[6]（Gradient Boosting Decision Tree）等，以及基于深度神经网络的模型。基于深度神经网络的模型普遍分为两种思路：基于表示的模型与基于交互的模型。基于表示的模型主要基于孪生神经网络[7]（Siamese）结构，“孪生”是通过共享参数而实现的，将两个文本映射到同一空间，再计算相似度，例如DSSM（Deep Structured Semantic Models）、CLSM[8]（Convolutional Latent Semantic Model）、ARC-I[9]（Architecture-I）等等；优点是结构简单，可以用于构建索引等场景，缺点是两个短文本的编码完全独立进行，无法考虑任何短文本内部之间的关联。基于交互的模型为了解决上述问题，将编码的过程加入了短文本内部之间的关联参数矩阵，更好地把握了语义焦点，能对上下文重要性进行更好的建模，例如ARC-II[9]（Architecture-II）、MatchPyramid[10]、MVLSTM[11]（Multi-variable Long Short-Term Memory）等。有监督方法可以在特定的场景下通过有标签数据来定制化相似度模型，从而提高准确率；但是有标签数据的获取成本太大，学习的时间复杂度也很高。</p>
<h4 id="有监督-无监督的相似度计算"><a href="#有监督-无监督的相似度计算" class="headerlink" title="有监督+无监督的相似度计算"></a>有监督+无监督的相似度计算</h4><p>有监督和无监督的混合方法鉴于无监督学习和有监督学习的优缺点，将它们结合到一起，提高了无监督学习的准确率并且降低了有监督学习的时间成本。这种混合方法其实是一种迁移学习（迁移学习是一种机器学习的方法，指的是一个预训练的模型被重新用在另一个任务中，一般两种任务之间需要有一定的相似性和关联性），随着近年来研究的进行，其效果越来越突显出来，例如2018年google提出来的Bert模型[4]以及2019年google再次提出来的XLNet模型[3]，都在各项自然语言处理任务（包括短文本相似度）上取得了颠覆性的突破。其中，Bert借用了Transformer结构中的encoding部分来对文本进行编码，其基于Self-Attention机制，并引入了Positional Encoding，既能够有效地捕捉文本字符间的位置关系，又能并行计算以提高计算效率，只需要在大量的无监督数据集上进行模型的预训练，并在特定任务集合上做少量的fine-tune，便可以达到一个很不错的效果。当然，Bert的预训练过程也并不是完美无缺的，Bert的预训练语言模型目标函数的计算过程强行做了独立性假设，并且基于随机概率MASK的预训练文本和基于具体任务的整句文本fine-tune的过程，具有明显的不一致性。综上考虑，XLNet被提出，为了解决双向上下文的问题，XLNet引入了排列（permutation）语言模型。排列语言模型在预测时，需要预测目标的位置信息，因此将传统的Self-Attention修改为Two-Stream Self-Attention来捕捉位置信息；XLNet还借鉴了Transformer-XL的优点，它对于很长的上下文的处理是要优于传统的Transformer的。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1].  Hofmann T. Probabilistic latent semantic analysis[J]. arXiv preprint arXiv:1301.6705, 2013.</p>
<p>[2].  Kusner M, Sun Y, Kolkin N, et al. From word embeddings to document distances[C]. International conference on machine learning. 2015: 957-966.</p>
<p>[3].  Yang Z, Dai Z, Yang Y, et al. Xlnet: Generalized autoregressive pretraining for language understanding[C]. Advances in neural information processing systems. 2019: 5753-5763.</p>
<p>[4].  Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>[5].  Peters M E, Neumann M, Iyyer M, et al. Deep contextualized word representations[J]. arXiv preprint arXiv:1802.05365, 2018.</p>
<p>[6].  Ke G, Meng Q, Finley T, et al. Lightgbm: A highly efficient gradient boosting decision tree[C]. Advances in neural information processing systems. 2017: 3146-3154.</p>
<p>[7].  Chopra S, Hadsell R, LeCun Y. Learning a similarity metric discriminatively, with application to face verification[C]. 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05). IEEE, 2005, 1: 539-546.</p>
<p>[8].  Shen Y, He X, Gao J, et al. A latent semantic model with convolutional-pooling structure for information retrieval[C]. Proceedings of the 23rd ACM international conference on conference on information and knowledge management. 2014: 101-110.</p>
<p>[9].  Hu B, Lu Z, Li H, et al. Convolutional neural network architectures for matching natural language sentences[C]. Advances in neural information processing systems. 2014: 2042-2050.</p>
<p>[10].       Pang L, Lan Y, Guo J, et al. Text matching as image recognition[J]. arXiv preprint arXiv:1602.06359, 2016.</p>
<p>[11].       Wan S, Lan Y, Guo J, et al. A deep architecture for semantic matching with multiple positional sentence representations[J]. arXiv preprint arXiv:1511.08277, 2015.</p>
]]></content>
      <tags>
        <tag>调研报告</tag>
      </tags>
  </entry>
  <entry>
    <title>检索式开放领域对话系统研究综述梳理笔记</title>
    <url>/2021/03/14/%E6%A3%80%E7%B4%A2%E5%BC%8F%E5%BC%80%E6%94%BE%E9%A2%86%E5%9F%9F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<p>本文绘制了《基于深度学习的开放领域对话系统研究综述》一文的思维导图；针对检索式开放领域对话系统的相关知识进行了梳理并记录了学习笔记，以便日后查看和复习。</p>
<a id="more"></a>

<h3 id="思维导图"><a href="#思维导图" class="headerlink" title="思维导图"></a>思维导图</h3><p><img src="/2021/03/14/%E6%A3%80%E7%B4%A2%E5%BC%8F%E5%BC%80%E6%94%BE%E9%A2%86%E5%9F%9F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0/%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE" alt="思维导图"></p>
<h3 id="学习笔记"><a href="#学习笔记" class="headerlink" title="学习笔记"></a>学习笔记</h3><p><img src="/2021/03/14/%E6%A3%80%E7%B4%A2%E5%BC%8F%E5%BC%80%E6%94%BE%E9%A2%86%E5%9F%9F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0/p1.png" alt="学习笔记1"></p>
<p><img src="/2021/03/14/%E6%A3%80%E7%B4%A2%E5%BC%8F%E5%BC%80%E6%94%BE%E9%A2%86%E5%9F%9F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0/p2.png" alt="学习笔记2"></p>
<p><img src="/2021/03/14/%E6%A3%80%E7%B4%A2%E5%BC%8F%E5%BC%80%E6%94%BE%E9%A2%86%E5%9F%9F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0/p3.png" alt="学习笔记3"></p>
<p><img src="/2021/03/14/%E6%A3%80%E7%B4%A2%E5%BC%8F%E5%BC%80%E6%94%BE%E9%A2%86%E5%9F%9F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0/p4.png" alt="学习笔记4"></p>
<p><img src="/2021/03/14/%E6%A3%80%E7%B4%A2%E5%BC%8F%E5%BC%80%E6%94%BE%E9%A2%86%E5%9F%9F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0/p5.png" alt="学习笔记5"></p>
<p><img src="/2021/03/14/%E6%A3%80%E7%B4%A2%E5%BC%8F%E5%BC%80%E6%94%BE%E9%A2%86%E5%9F%9F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0/p6.png" alt="学习笔记6"></p>
<p><img src="/2021/03/14/%E6%A3%80%E7%B4%A2%E5%BC%8F%E5%BC%80%E6%94%BE%E9%A2%86%E5%9F%9F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%A0%94%E7%A9%B6%E7%BB%BC%E8%BF%B0%E6%A2%B3%E7%90%86%E7%AC%94%E8%AE%B0/p7.png" alt="学习笔记7"></p>
]]></content>
      <tags>
        <tag>开放领域对话系统</tag>
      </tags>
  </entry>
  <entry>
    <title>知识图谱调研</title>
    <url>/2020/11/22/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%B0%83%E7%A0%94/</url>
    <content><![CDATA[<p>最近由于项目需要接触到一些知识图谱领域的知识，为了方便日后进一步研究或应用，先进行了宏观上的调研，总结了本篇调研报告。</p>
<a id="more"></a>

<h2 id="知识图谱调研"><a href="#知识图谱调研" class="headerlink" title="知识图谱调研"></a>知识图谱调研</h2><h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><ol>
<li>知识图谱的定义</li>
<li>国内外研究现状</li>
<li>代表性的知识图谱库</li>
<li>知识图谱的架构</li>
<li>知识图谱的构建技术</li>
<li>知识图谱的存储</li>
</ol>
<hr>
<h4 id="知识图谱的定义"><a href="#知识图谱的定义" class="headerlink" title="知识图谱的定义"></a>知识图谱的定义</h4><ul>
<li><p>从学术的角度，知识图谱是结构化的语义知识库，用于以符号形式描述物理世界中的概念及其相互关系。其基本组成单位是“实体－关系－实体”三元组，以及实体及其相关属性－ 值对，实体间通过关系相互联结，构成网状的知识结构。</p>
</li>
<li><p>从实际应用的角度，可以简单地把知识图谱理解成多关系图（Multi-relational Graph），其一般包含多种类型的节点和多种类型的边。在知识图谱里，我们通常用”实体（Entity）“来表达图里的节点、用“关系（Relation）”来表达图里的“边”。实体指的是现实世界中的事物，比如人、地名、概念、公司等，关系则用来表达不同实体之间的某种联系，比如人-“居住在”-北京、张三和李四是“朋友”等等。</p>
</li>
<li><p>该定义有三方面的含义：</p>
<ol>
<li>知识图谱本身是一个具有属性的实体通过关系链接而形成的网状知识库。从图的概念来看，知识图谱是对物理世界的一种符号表达；</li>
<li>知识图谱的研究价值在于：它是构建在当前Web基础上的一层覆盖网络，知识图谱的目标是在当前网页之间建立概念的链接，从而将互联网上的知识进行整合，成为“知识”；</li>
<li>知识图谱的应用价值在于，它可以帮助人们准确的锁定自己想要检索的内容，而不必从海量网页中人工过滤。</li>
</ol>
</li>
</ul>
<hr>
<h4 id="国内外研究现状"><a href="#国内外研究现状" class="headerlink" title="国内外研究现状"></a>国内外研究现状</h4><p>目前，国内外学者对其的研究主要集中于知识图谱的关键技术：</p>
<ul>
<li>知识表示；</li>
<li>知识图谱构建；</li>
<li>知识图谱应用。</li>
</ul>
<ol>
<li>知识表示的发展过程主要分为三个阶段：</li>
</ol>
<ul>
<li>比较早时期采用符号逻辑的方式；</li>
<li>2000年以后出现的语义网对知识进行表示的方式，包括RDF、OWL等；</li>
<li>目前比较流行的采用表示学习方式，将知识学习成低维稠密的向量，通过向量间的关系可以在某种程度上反映知识之间的关系。</li>
</ul>
<ol start="2">
<li><p>知识图谱的构建主要是三个步骤：</p>
<ul>
<li><p>知识抽取</p>
<ul>
<li>知识抽取的目标就是从数据源中抽取出实体、关系及属性等信息，进而进行下一步的操作，所以这一项工作更准确地说应该叫做信息抽取，因为这一步骤并不能得到我们最终想要的“知识”；</li>
<li>这一步骤涉及到的主要技术包括：实体抽取、关系抽取以及属性抽取。</li>
</ul>
</li>
<li><p>知识融合</p>
<ul>
<li>知识融合过程中，需要对第一步中抽取出的三元组信息进行处理，比如将指向同一类实体的三元组进行合并，或者将同名不同含义的实体进行区分；</li>
<li>这一步骤涉及的技术是指代消解、实体消歧和知识合并。</li>
</ul>
</li>
<li><p>知识加工</p>
<ul>
<li>知识加工是将前两步骤处理后的数据转换为结构化的知识；</li>
<li>主要包括三方面内容：本体构建，知识推理和质量评估。</li>
</ul>
</li>
</ul>
</li>
<li><p>知识图谱的应用主要有三个方向：</p>
<ul>
<li>语义搜索，利用知识图谱所具有的良好定义的结构形式，以有向图的方式提供满足用户需求的结构化语义内容；</li>
<li>知识问答，基于知识库的问答，通过对问句的语义分析，转换成结构化的查询语句，在已有结构化的知识库上获取答案；</li>
<li>知识驱动的大数据分析与决策。</li>
</ul>
</li>
<li><p>根据覆盖范围而言，知识图谱也可分为：</p>
<ul>
<li>开放域通用知识图谱<ul>
<li>开放通用知识图谱注重广度，强调融合更多的实体，较垂直行业知识图谱而言，其准确度不够高，并且受概念范围的影响，很难借助本体库对公理、规则以及约束条件的支持能力规范其实体、属性、实体间的关系等。</li>
</ul>
</li>
<li>垂直行业知识图谱<ul>
<li>行业知识图谱通常需要依靠特定行业的数据来构建，具有特定的行业意义。行业知识图谱中，实体的属性与数据模式往往比较丰富，需要考虑到不同的业务场景与使用人员。</li>
</ul>
</li>
</ul>
</li>
</ol>
<hr>
<h4 id="代表性的知识图谱库"><a href="#代表性的知识图谱库" class="headerlink" title="代表性的知识图谱库"></a>代表性的知识图谱库</h4><p><img src="/2020/11/22/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%B0%83%E7%A0%94/p1.png" alt="代表性知识图谱库"></p>
<hr>
<h4 id="知识图谱的架构"><a href="#知识图谱的架构" class="headerlink" title="知识图谱的架构"></a>知识图谱的架构</h4><ul>
<li><p>逻辑架构</p>
<ul>
<li><p>所谓逻辑架构，指的是图谱的双层结构：数据层和模式层。</p>
</li>
<li><p>在数据层，知识以事实（fact）为单位存储在图数据库中。事实一般以三元组的方式进行描述，例如“实体-关系-实体”、“实体-属性-属性值”，借此，图数据库中所有的数据将组成庞大的关系网络，形成所谓的“图谱”。</p>
</li>
<li><p>模式层建立在数据层之上，按照字面意思理解就是：对数据层的知识设置一种过滤模式，从而减少知识的冗余。举个例子，比如一个人可能有两个名字，在数据层，不同的名字可能对应了不同的属性和关系，而模式层就是把这实质上是同一个人的实体及其关系进行融合（知识融合），去除不必要的冗余。</p>
</li>
</ul>
<p><img src="/2020/11/22/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%B0%83%E7%A0%94/p2.png" alt="逻辑架构"></p>
</li>
<li><p>技术架构</p>
<p>知识图谱的整体技术流程如下图所示：</p>
<p><img src="/2020/11/22/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%B0%83%E7%A0%94/p3.png" alt="知识图谱技术流程图"></p>
<p>橘色部分代表着知识图谱的构建及迭代过程：知识抽取、知识融合、知识加工。一般人们会把知识图谱的构建分为自顶而下和自底而上两种不同的形式，随着技术的不断成熟，现在大都采用自底而上的方法，即从数据中提取资源模式，通过一定的技术手段，选择其中置信度较高的部分，加入到知识图谱中去。</p>
</li>
</ul>
<hr>
<h4 id="知识图谱的构建技术"><a href="#知识图谱的构建技术" class="headerlink" title="知识图谱的构建技术"></a>知识图谱的构建技术</h4><p>上文提到，知识图谱的构建及迭代主要是三个步骤，下面就每个步骤进行说明。</p>
<ol>
<li><p>知识抽取（Knowledge Extraction）</p>
<p>知识抽取的目标就是从数据源中抽取出实体、关系及属性等信息，进而进行下一步的操作，所以这一项工作更准确地说应该叫做信息抽取，因为这一步骤并不能得到我们最终想要的“知识”。相应地，这一步骤涉及到的主要技术包括：实体抽取、关系抽取以及属性抽取。</p>
<ul>
<li><p>实体抽取</p>
<p>即所谓的命名实体识别（Named Entity Recognition，NER），指的就是如何从数据，特别是文本数据中准确获得其中的实体信息。实体的类型主要包括三大类七小类：</p>
<ul>
<li>实体类（包括人名，地名，机构名）；</li>
<li>时间类（日期，时间）</li>
<li>数字类（货币、百分比）</li>
</ul>
<p>具体到特定领域，还可以对实体类别进行扩充。</p>
</li>
<li><p>关系抽取</p>
<p>实体抽取获得了图数据库中的节点，那么关系抽取就是获得节点之间的连线的过程。<br>在早期的研究中，关系抽取同样基于规则，但这种方法本身的局限注定基于规则建立模型不是长久之计；后来采用统计机器学习的方法，得到了不错的效果，但需要大量人工标注的语料，这对于特定领域的关系抽取来说，工作量无疑是巨大的；再后来的基于半监督、无监督以及自监督模型，对模型的通用性有了一些提高，但整体来说，由于自然语言的复杂性，这一领域的学者仍然是任重而道远。</p>
</li>
<li><p>属性抽取</p>
<p>如果把实体的属性值看作是一种特殊的实体，那么属性抽取实际上也是一种关系抽取。 百科类网站提供的半结构化数据是通用领域属性抽取研究的主要数据来源，但具体到特定的应用领域，涉及大量的非结构化数据，属性抽取仍然是一个巨大的挑战。</p>
</li>
</ul>
</li>
<li><p>知识融合（Knowledge Fusion）</p>
<p>上一步抽取出的三元组信息存入数据库后，具备了初步的“知识”，但这些知识仍然不能直接利用。例如，对于前美国总统奥巴马，我们可能用不同的词来称呼他，在实体识别中这些称呼都被看作是独立的实体，那么我们就应该把这些指向同一类实体的三元组进行合并。再举一个例子，同名同姓的两个不同的人，如何区分他们呢？这就涉及下面的技术。</p>
<ul>
<li><p>指代消解</p>
<p>这个词可能不是很好理解，但理解它所代表的意思即可：消解那些指代不同实体的内容，这一技术就是处理对同一实体的不同语言描述问题（如上文对奥巴马的不同称呼）。利用这一技术，就可以将这些看似不同的实体归纳到一个统一的实体。这一技术本身也具有很多“称呼”：实体对齐，实体匹配，实体同义等。</p>
</li>
<li><p>实体消歧</p>
<p>相比而言，这个词就好理解多了，其作用也简单，就是对具有相同名称的实体进行区分。例如两个人同名，那就通过性别、工作、兴趣爱好等其他属性进行区分。</p>
</li>
<li><p>知识合并</p>
<p>知识合并主要就是从结构化数据或者第三方库中获取知识并融合进图谱的过程。</p>
</li>
</ul>
</li>
<li><p>知识加工（Knowledge Processing）</p>
<p>严格来讲，经过前两步骤处理后的数据仍不能称得上是“知识”，只能算是事实表达的集合，要想获得结构化的知识，还需要经过知识加工环节。知识加工主要包括3方面内容： 本体构建，知识推理和质量评估。</p>
<ul>
<li><p>本体构建</p>
<p>本体最早是一个哲学上的概念，在知识图谱上引用本体构建这一技术，目的是对相关领域的知识进行总结提取，确定在该领域内得到共识的术语，然后以格式规范的定义来描述这些内容。可以这样理解，前两阶段提取出来的实体及其属性关系，是本体的具体表现；而本体就是对它们的概括总结。</p>
</li>
<li><p>知识推理</p>
<p>知识推理指的是从已有的知识中推理出新的知识。例如康熙是雍正的父亲，雍正是乾隆的父亲，那么通过康熙和乾隆这两个实体之间通过知识推理，就可以获得他们之间是祖孙关系。</p>
</li>
<li><p>质量评估</p>
<p>顾名思义，质量评估就是对写入知识图谱中的知识做最后的判断，以提高知识库中内容的准确性。</p>
</li>
</ul>
</li>
<li><p>知识更新（Knowledge Update）</p>
<p>知识图谱并不是一成不变的，随着人类知识的增加，图谱的内容也要定期进行更新，以满足最新的知识需求。更新的方式有两种，一种是“打补丁”，即增量更新，将新增加的人类知识经过处理加入到知识图谱中；另一种是“推倒重建”，即全局更新，就是从零开始重建新图谱。事实上，无论哪种方法都是一件消耗巨大的工作。</p>
</li>
</ol>
<hr>
<h4 id="知识图谱的存储"><a href="#知识图谱的存储" class="headerlink" title="知识图谱的存储"></a>知识图谱的存储</h4><p>知识图谱主要有两种存储方式：一种是基于RDF的存储；另一种是基于图数据库的存储。它们之间的区别如下图所示。RDF一个重要的设计原则是数据的易发布、共享，图数据库则把重点放在了高效的图查询和搜索上；此外，RDF以三元组的方式来存储数据而且不包含属性信息，但图数据库一般以属性图为基本的表示形式，所以实体和关系可以包含属性，这就意味着更容易表达现实的业务场景。</p>
<p><img src="/2020/11/22/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%B0%83%E7%A0%94/p4.png" alt="区别"></p>
<p>根据最新的统计，图数据库仍然是增长最快的存储系统。相反，关系型数据库的增长基本保持在一个稳定的水平。下图列出了常用的图数据库系统以及他们最新使用情况的排名。 其中Neo4j系统目前仍是使用率最高的图数据库，它拥有活跃的社区，而且系统本身的查询效率高，但唯一的不足就是不支持准分布式；相反，OrientDB和JanusGraph（原Titan）支持分布式，但这些系统相对较新，社区不如Neo4j活跃，这也就意味着使用过程当中不可避免地会遇到一些刺手的问题。如果选择RDF的存储系统，Jena或许一个比较不错的选择。</p>
<p><img src="/2020/11/22/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E8%B0%83%E7%A0%94/p5.png" alt="数据库"></p>
]]></content>
      <tags>
        <tag>调研报告</tag>
      </tags>
  </entry>
  <entry>
    <title>知识库、数据集调研报告</title>
    <url>/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/</url>
    <content><![CDATA[<p>关于Freebase、google knowledge graph、Wikidata知识库和《Concretely Annotated Corpora》论文数据集的调研报告。</p>
<a id="more"></a>

<h2 id="知识库调研"><a href="#知识库调研" class="headerlink" title="知识库调研"></a>知识库调研</h2><h3 id="Freebase"><a href="#Freebase" class="headerlink" title="Freebase"></a>Freebase</h3><p>2010年7月16日被谷歌收购。2014年12月16日，Google宣布将在六个月后关闭Freebase，并帮助将数据从Freebase迁移至<a href="https://zh.wikipedia.org/wiki/%E7%BB%B4%E5%9F%BA%E6%95%B0%E6%8D%AE">维基数据</a>。2015年12月16日，Google正式发布<a href="https://developers.google.com/knowledge-graph/">知识图谱API</a>页面，用以替代Freebase API。Freebase.com于2016年5月2日正式关闭。</p>
<h3 id="google-knowledge-graph"><a href="#google-knowledge-graph" class="headerlink" title="google knowledge graph"></a>google knowledge graph</h3><ol>
<li><p>网址：<a href="https://developers.google.com/knowledge-graph/">https://developers.google.com/knowledge-graph/</a></p>
</li>
<li><p>有API接口，也可以在线访问数据</p>
</li>
<li><p>返回数据格式是json</p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;@context&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;@vocab&quot;</span>: <span class="string">&quot;http://schema.org/&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;goog&quot;</span>: <span class="string">&quot;http://schema.googleapis.com/&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;resultScore&quot;</span>: <span class="string">&quot;goog:resultScore&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;detailedDescription&quot;</span>: <span class="string">&quot;goog:detailedDescription&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;EntitySearchResult&quot;</span>: <span class="string">&quot;goog:EntitySearchResult&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;kg&quot;</span>: <span class="string">&quot;http://g.co/kg&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;@type&quot;</span>: <span class="string">&quot;ItemList&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;itemListElement&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;@type&quot;</span>: <span class="string">&quot;EntitySearchResult&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;result&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;@id&quot;</span>: <span class="string">&quot;kg:/m/0dl567&quot;</span>,		<span class="comment">//标识，对应的entity在 Freebase 中的mid。</span></span><br><span class="line">        <span class="attr">&quot;name&quot;</span>: <span class="string">&quot;Taylor Swift&quot;</span>,		<span class="comment">//姓名</span></span><br><span class="line">        <span class="attr">&quot;@type&quot;</span>: [</span><br><span class="line">          <span class="string">&quot;Thing&quot;</span>,</span><br><span class="line">          <span class="string">&quot;Person&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;description&quot;</span>: <span class="string">&quot;Singer-songwriter&quot;</span>,		<span class="comment">//简要描述</span></span><br><span class="line">        <span class="attr">&quot;image&quot;</span>: &#123;								<span class="comment">//可供识别的图片</span></span><br><span class="line">          <span class="attr">&quot;contentUrl&quot;</span>: <span class="string">&quot;https://t1.gstatic.com/images?q=tbn:ANd9GcQmVDAhjhWnN2OWys2ZMO3PGAhupp5tN2LwF_BJmiHgi19hf8Ku&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;url&quot;</span>: <span class="string">&quot;https://en.wikipedia.org/wiki/Taylor_Swift&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;license&quot;</span>: <span class="string">&quot;http://creativecommons.org/licenses/by-sa/2.0&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;detailedDescription&quot;</span>: &#123;				<span class="comment">//详细描述</span></span><br><span class="line">          <span class="attr">&quot;articleBody&quot;</span>: <span class="string">&quot;Taylor Alison Swift is an American singer-songwriter and actress. Raised in Wyomissing, Pennsylvania, she moved to Nashville, Tennessee, at the age of 14 to pursue a career in country music. &quot;</span>,</span><br><span class="line">          <span class="attr">&quot;url&quot;</span>: <span class="string">&quot;http://en.wikipedia.org/wiki/Taylor_Swift&quot;</span>,</span><br><span class="line">          <span class="attr">&quot;license&quot;</span>: <span class="string">&quot;https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">&quot;url&quot;</span>: <span class="string">&quot;http://taylorswift.com/&quot;</span>		<span class="comment">//个人主页</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;resultScore&quot;</span>: <span class="number">4850</span>		<span class="comment">//与搜索内容的匹配程度</span></span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>注意</strong>：实体中的大部分信息是直接显示在Google相关搜索的右侧栏wiki中的，不够丰富。</p>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/Taylor_Swift.png" alt="Taylor_Swift"></p>
</li>
<li><p><strong>局限</strong>：知识图搜索API仅返回单个匹配实体，而不是互连实体的图，不能反映实体之间的关系。</p>
</li>
<li><p>在线搜索举例：</p>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/karenBass.png" alt="karen Bass"></p>
<ul>
<li><p>各参数说明：</p>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/google_KG_statement.jpg" alt="google_KG_statement"></p>
</li>
<li><p>返回结果是json格式：</p>
</li>
</ul>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;@context&quot;</span>: &#123;</span><br><span class="line">    <span class="attr">&quot;resultScore&quot;</span>: <span class="string">&quot;goog:resultScore&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;EntitySearchResult&quot;</span>: <span class="string">&quot;goog:EntitySearchResult&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;kg&quot;</span>: <span class="string">&quot;http://g.co/kg&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;detailedDescription&quot;</span>: <span class="string">&quot;goog:detailedDescription&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;goog&quot;</span>: <span class="string">&quot;http://schema.googleapis.com/&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;@vocab&quot;</span>: <span class="string">&quot;http://schema.org/&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;@type&quot;</span>: <span class="string">&quot;ItemList&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;itemListElement&quot;</span>: [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;resultScore&quot;</span>: <span class="number">16844.9296875</span>,</span><br><span class="line">      <span class="attr">&quot;@type&quot;</span>: <span class="string">&quot;EntitySearchResult&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;result&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;detailedDescription&quot;</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">&quot;url&quot;</span>: <span class="string">&quot;https://en.wikipedia.org/wiki/Karen_Bass&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;articleBody&quot;</span>: <span class="string">&quot;Karen Ruth Bass is an American politician serving as the U.S. Representative for California&#x27;s 37th congressional district since 2011. &quot;</span>,</span><br><span class="line">            <span class="attr">&quot;license&quot;</span>: <span class="string">&quot;https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;inLanguage&quot;</span>: <span class="string">&quot;en&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;@id&quot;</span>: <span class="string">&quot;kg:/m/027g9m6&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;description&quot;</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">&quot;@value&quot;</span>: <span class="string">&quot;U.S. Representative&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;@language&quot;</span>: <span class="string">&quot;en&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">&quot;@language&quot;</span>: <span class="string">&quot;zh&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;@value&quot;</span>: <span class="string">&quot;美国众议院议员&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">&quot;@language&quot;</span>: <span class="string">&quot;zh-TW&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;@value&quot;</span>: <span class="string">&quot;美國眾議院議員&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;@type&quot;</span>: [</span><br><span class="line">          <span class="string">&quot;Thing&quot;</span>,</span><br><span class="line">          <span class="string">&quot;Person&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        <span class="attr">&quot;url&quot;</span>: <span class="string">&quot;http://www.karenbass.com/&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;name&quot;</span>: [</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">&quot;@language&quot;</span>: <span class="string">&quot;en&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;@value&quot;</span>: <span class="string">&quot;Karen Bass&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">&quot;@value&quot;</span>: <span class="string">&quot;凯伦·巴斯&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;@language&quot;</span>: <span class="string">&quot;zh&quot;</span></span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            <span class="attr">&quot;@value&quot;</span>: <span class="string">&quot;凱倫·巴斯&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;@language&quot;</span>: <span class="string">&quot;zh-TW&quot;</span></span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



</li>
</ol>
<h3 id="Wikidata"><a href="#Wikidata" class="headerlink" title="Wikidata"></a>Wikidata</h3><ol>
<li><p>首页网址：<a href="https://www.wikidata.org/wiki/Wikidata:Main_Page">https://www.wikidata.org/wiki/Wikidata:Main_Page</a></p>
</li>
<li><p>访问Wikidata的数据：<a href="https://www.wikidata.org/wiki/">https://www.wikidata.org/wiki/</a> + 人物标识码</p>
<p>如：<a href="https://www.wikidata.org/wiki/Q461739">https://www.wikidata.org/wiki/Q461739</a></p>
<ul>
<li>这一种方法和google knowledge graph的request差不多，都是用URL来访问</li>
<li>用这种方法来访问并不是连接到维基百科的某个词条，而是一个单独的网页。这个不需要自己的apikey。感觉比较方便。并且这时返回的是一个结构化的网页，阅读性较好。如下图，会有很多属性-值这种关系</li>
</ul>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/KarenBass2.png" alt="karenBass2"></p>
<ul>
<li>不过也可能不全，比如下图的核心小组成员，其实只列出了一条。</li>
</ul>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/karenBass3.png" alt="karenBass3"></p>
</li>
<li><p>数据转储方式</p>
<p>这里提供两种方式：</p>
<h4 id="（1）API接口"><a href="#（1）API接口" class="headerlink" title="（1）API接口"></a>（1）API接口</h4><p>wikidata提供了API接口，用于通过实体的标识码获取实体的详细信息，主要包括与相关的其他实体的关系信息。 此API可以将结果以多种格式返回，例如以HTTP GET的方式获取标识码为Q461739的实体的详细信息并指定以json格式返回的url为：</p>
<p><a href="https://www.wikidata.org/wiki/Special:EntityData/Q461739.json">https://www.wikidata.org/wiki/Special:EntityData/Q461739.json</a></p>
<p>json格式的返回数据为：</p>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/KarenBass4.png" alt="karenBass4"></p>
<p>用在线json格式化工具可将其格式化：</p>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/KarenBass5.png" alt="karenBass5"></p>
<p>其中，json格式的entity顶级字段有：</p>
<ul>
<li>id: 实体标识码。</li>
<li>type: 实体类型。</li>
<li>labels: 不同语言描述的实体标签（姓名）。</li>
<li>descriptions: 不同语言的实体描述（类似谷歌知识图谱中的简要描述，如American politician）。</li>
<li>aliases: 不同语言描述的实体别名。</li>
<li>claims: 以属性分组的实体声明(claims)或者陈述(statements)。</li>
<li>sitelinks: 各种网站上关于此实体的描述。</li>
<li>lastrevid: 当前json文件的版本。</li>
<li>modified: 当前json文件的发布日期。</li>
</ul>
<p><strong>注意：</strong></p>
<ul>
<li><p>我们最关心的应该是“claims”，这包含了实体的具体属性关系，一个实体可以有多个属性关系。</p>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/claims.png" alt="claims"></p>
</li>
<li><p>claim 包含一条主体信息(main Snak)以及一些修饰信息(qualifier Snaks)。</p>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/claims3.png" alt="claims3"></p>
</li>
<li><p>statement是含有参考资料（reference）的claim。</p>
</li>
<li><p>每个claim总是与一个属性(property)关联(claim是关于此property的)。并且在一个实体中可以有多条claim与同一property关联。</p>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/claims2.png" alt="claims2"></p>
</li>
<li><p>claim含有以下字段:</p>
<ul>
<li>id: 识别码，只能保证当前数据库中唯一，不包含其他信息。</li>
<li>type: claim的类型，目前只有statement和claim两种。</li>
<li>mainsnak: 如果claim含有type值，那么它具有mainsnak字段包含与property相关的主体信息。</li>
<li>rank: 表示claim是否应该显示在查询结果中，为preferred, normal 或者 deprecated.</li>
<li>qualifiers: 修饰信息，一般为主体信息的上下文信息，每一条都与一个属性(property)关联。</li>
<li>references: 如果claim是statement，那么会有一个参考资料的列表。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>   <strong>解析claims：</strong></p>
<ul>
<li><p>json字段，除了claims顶级字段，其它信息都可以直接提取利用。</p>
</li>
<li><p>claims字段下为一个字典，字典的键为属性(property)，示例中只有一个key：P21。通过属性页查询url：<a href="https://www.wikidata.org/wiki/Property:P21%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%9F%A5%E9%81%93%E6%AD%A4%E5%B1%9E%E6%80%A7%E8%A1%A8%E7%A4%BA%E6%80%A7%E5%88%AB%E3%80%82">https://www.wikidata.org/wiki/Property:P21，可以知道此属性表示性别。</a></p>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/claims4.png" alt="claims4"></p>
</li>
<li><p>与此属性关联的claim只有一个，mainsnak为此claim的主体信息，datavalue中的value为”Q6581072”，通过<a href="https://www.wikidata.org/wiki/Q6581072%E5%8F%AF%E6%9F%A5%E8%AF%A2%E5%88%B0%E8%AF%A5claim%E8%A1%A8%E7%A4%BA%E5%AE%9E%E4%BD%93%E7%9A%84%E6%80%A7%E5%88%AB%E4%B8%BA%E5%A5%B3%E3%80%82datavalue%E4%B9%9F%E5%8F%AF%E4%BB%A5%E4%B8%BA%E5%85%B6%E5%AE%83%E5%85%B3%E8%81%94%E5%AE%9E%E4%BD%93%E6%8F%90%E4%BE%9B%E5%AE%9E%E4%BD%93id%E3%80%82">https://www.wikidata.org/wiki/Q6581072可查询到该claim表示实体的性别为女。datavalue也可以为其它关联实体提供实体id。</a></p>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/claims5.png" alt="claims5"></p>
</li>
<li><p>另外此claim的type为statment，因此含有一个参考资料列表-references。</p>
</li>
</ul>
<p>   <strong>解析难点：</strong></p>
<ul>
<li>属性都是以属性ID表示的，不能直接解析属性的含义</li>
<li>关联实体是以实体ID表示的，需要多次的查询-解析</li>
<li>不同种类实体含有的属性不同</li>
</ul>
<p>   <strong>其他格式：</strong></p>
<p>   用<a href="https://www.wikidata.org/wiki/Special:EntityData/Q461739.rdf%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E4%B8%8B%E8%BD%BD%E8%AF%A5%E5%AE%9E%E4%BD%93%E7%9A%84rdf%E5%8E%8B%E7%BC%A9%E5%8C%85%EF%BC%8C%E5%8E%8B%E7%BC%A9%E5%8C%85%E9%87%8C%E9%9D%A2%E5%8C%85%E5%90%AB%E5%90%84%E4%B8%AA%E5%9B%BD%E5%AE%B6%E5%9C%B0%E5%8C%BA%E6%96%87%E5%AD%97%E7%9A%84rdf%E3%80%82">https://www.wikidata.org/wiki/Special:EntityData/Q461739.rdf可以直接下载该实体的rdf压缩包，压缩包里面包含各个国家地区文字的rdf。</a></p>
<h4 id="（2）下载完整数据库"><a href="#（2）下载完整数据库" class="headerlink" title="（2）下载完整数据库"></a>（2）下载完整数据库</h4><p>   Wikidata提供了完整的数据库下载，因此可以下载完整的数据库（<a href="https://dumps.wikimedia.org/wikidatawiki/entities/%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E6%90%AD%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%AE%9E%E4%BD%93%E6%90%9C%E7%B4%A2%E6%9C%8D%E5%8A%A1%E3%80%82%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4%E6%98%AF%EF%BC%9A">https://dumps.wikimedia.org/wikidatawiki/entities/），然后搭建自己的实体搜索服务。具体步骤是：</a></p>
<ul>
<li><p>数据下载</p>
</li>
<li><p>数据导入</p>
</li>
<li><p>搭建搜索服务</p>
</li>
</ul>
<p><img src="/2021/01/03/%E7%9F%A5%E8%AF%86%E5%BA%93%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86%E8%B0%83%E7%A0%94%E6%8A%A5%E5%91%8A/database.png" alt="database"></p>
<h2 id="论文调研"><a href="#论文调研" class="headerlink" title="论文调研"></a>论文调研</h2><p>《Concretely Annotated Corpora》开发了CONCRETE来记录和共享注释文档。</p>
<p>主页见<a href="https://hltcoe.github.io/%E3%80%82">https://hltcoe.github.io/。</a></p>
<p>他给出了一个serif_dog-bites-man.concrete，可以用concrete中相关的工具查看文档内容：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">concrete-inspect.py --text serif_dog-bites-man.concrete</span><br></pre></td></tr></table></figure>

<p>该命令将通信文本输出到控制台。在这个案例中，该文本是采用SGML格式的简短文章：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;DOC id&#x3D;&quot;dog-bites-man&quot; type&#x3D;&quot;other&quot;&gt;</span><br><span class="line">&lt;HEADLINE&gt;</span><br><span class="line">Dog Bites Man</span><br><span class="line">&lt;&#x2F;HEADLINE&gt;</span><br><span class="line">&lt;TEXT&gt;</span><br><span class="line">&lt;P&gt;</span><br><span class="line">John Smith, manager of ACMÉ INC, was bit by a dog on March 10th, 2013.</span><br><span class="line">&lt;&#x2F;P&gt;</span><br><span class="line">&lt;P&gt;</span><br><span class="line">He died!</span><br><span class="line">&lt;&#x2F;P&gt;</span><br><span class="line">&lt;P&gt;</span><br><span class="line">John&#39;s daughter Mary expressed sorrow.</span><br><span class="line">&lt;&#x2F;P&gt;</span><br><span class="line">&lt;&#x2F;TEXT&gt;</span><br><span class="line">&lt;&#x2F;DOC&gt;</span><br></pre></td></tr></table></figure>

<p>现在运行以下命令来检查存储在该Communication中的某些注释：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">concrete-inspect.py --ner --pos --dependency serif_dog-bites-man.concrete</span><br></pre></td></tr></table></figure>

<p>此命令以<a href="http://ufal.mff.cuni.cz/conll2009-st/task-description.html">类似于CoNLL</a>的列格式显示标记化，词性标记，命名实体标记和依赖项解析：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">INDEX       TOKEN   POS     NER     HEAD    DEPREL</span><br><span class="line">-----       -----   ---     ---     ----    ------</span><br><span class="line">1   John    NNP     PER     2       compound</span><br><span class="line">2   Smith   NNP     PER     10      nsubjpass</span><br><span class="line">3   ,       ,</span><br><span class="line">4   manager NN              2       appos</span><br><span class="line">5   of      IN              7       case</span><br><span class="line">6   ACMÉ    NNP     ORG     7       compound</span><br><span class="line">7   INC     NNP     ORG     4       nmod</span><br><span class="line">8   ,       ,</span><br><span class="line">9   was     VBD             10      auxpass</span><br><span class="line">10  bit     NN              0       ROOT</span><br><span class="line">11  by      IN              13      case</span><br><span class="line">12  a       DT              13      det</span><br><span class="line">13  dog     NN              10      nmod</span><br><span class="line">14  on      IN              15      case</span><br><span class="line">15  March   DATE-NNP                13      nmod</span><br><span class="line">16  10th    JJ              15      amod</span><br><span class="line">17  ,       ,</span><br><span class="line">18  2013    CD              13      amod</span><br><span class="line">19  .       .</span><br><span class="line"></span><br><span class="line">1   He      PRP             2       nsubj</span><br><span class="line">2   died    VBD             0       ROOT</span><br><span class="line">3   !       .</span><br><span class="line"></span><br><span class="line">1   John    NNP     PER     3       nmod:poss</span><br><span class="line">2   &#39;s      POS             1       case</span><br><span class="line">3   daughter        NN              5       dep</span><br><span class="line">4   Mary    NNP     PER     5       nsubj</span><br><span class="line">5   expressed       VBD             0       ROOT</span><br><span class="line">6   sorrow  NN              5       dobj</span><br><span class="line">7   .       .</span><br></pre></td></tr></table></figure>

<p>还有一些其他的显示格式，比如ID、元数据、section、实体、提取等。其中，实体的格式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">  Entity 0-0:</span><br><span class="line">      EntityMention 0-0-0:</span><br><span class="line">          tokens:     John Smith</span><br><span class="line">          text:       John Smith</span><br><span class="line">          entityType: PER</span><br><span class="line">          phraseType: PhraseType.NAME</span><br><span class="line">      EntityMention 0-0-1:</span><br><span class="line">          tokens:     John Smith , manager of ACMÉ INC ,</span><br><span class="line">          text:       John Smith, manager of ACMÉ INC,</span><br><span class="line">          entityType: PER</span><br><span class="line">          phraseType: PhraseType.APPOSITIVE</span><br><span class="line">          child EntityMention #0:</span><br><span class="line">              tokens:     John Smith</span><br><span class="line">              text:       John Smith</span><br><span class="line">              entityType: PER</span><br><span class="line">              phraseType: PhraseType.NAME</span><br><span class="line">          child EntityMention #1:</span><br><span class="line">              tokens:     manager of ACMÉ INC</span><br><span class="line">              text:       manager of ACMÉ INC</span><br><span class="line">              entityType: PER</span><br><span class="line">              phraseType: PhraseType.COMMON_NOUN</span><br><span class="line">      EntityMention 0-0-2:</span><br><span class="line">          tokens:     manager of ACMÉ INC</span><br><span class="line">          text:       manager of ACMÉ INC</span><br><span class="line">          entityType: PER</span><br><span class="line">          phraseType: PhraseType.COMMON_NOUN</span><br><span class="line">      EntityMention 0-0-3:</span><br><span class="line">          tokens:     He</span><br><span class="line">          text:       He</span><br><span class="line">          entityType: PER</span><br><span class="line">          phraseType: PhraseType.PRONOUN</span><br><span class="line">      EntityMention 0-0-4:</span><br><span class="line">          tokens:     John</span><br><span class="line">          text:       John</span><br><span class="line">          entityType: PER.Individual</span><br><span class="line">          phraseType: PhraseType.NAME</span><br><span class="line"></span><br><span class="line">  Entity 0-1:</span><br><span class="line">      EntityMention 0-1-0:</span><br><span class="line">          tokens:     ACMÉ INC</span><br><span class="line">          text:       ACMÉ INC</span><br><span class="line">          entityType: ORG</span><br><span class="line">          phraseType: PhraseType.NAME</span><br><span class="line"></span><br><span class="line">  Entity 0-2:</span><br><span class="line">      EntityMention 0-2-0:</span><br><span class="line">          tokens:     John &#39;s daughter Mary</span><br><span class="line">          text:       John&#39;s daughter Mary</span><br><span class="line">          entityType: PER.Individual</span><br><span class="line">          phraseType: PhraseType.NAME</span><br><span class="line">          child EntityMention #0:</span><br><span class="line">              tokens:     Mary</span><br><span class="line">              text:       Mary</span><br><span class="line">              entityType: PER</span><br><span class="line">              phraseType: PhraseType.OTHER</span><br><span class="line">      EntityMention 0-2-1:</span><br><span class="line">          tokens:     daughter</span><br><span class="line">          text:       daughter</span><br><span class="line">          entityType: PER</span><br><span class="line">          phraseType: PhraseType.COMMON_NOUN</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Entity Set 1 (Serif: doc-values):</span><br><span class="line">  Entity 1-0:</span><br><span class="line">      EntityMention 1-0-0:</span><br><span class="line">          tokens:     March 10th , 2013</span><br><span class="line">          text:       March 10th, 2013</span><br><span class="line">          entityType: TIMEX2.TIME</span><br><span class="line">          phraseType: PhraseType.OTHER</span><br></pre></td></tr></table></figure>

<p>说明文档见<a href="https://concrete-python.readthedocs.io/en/stable/README.html">https://concrete-python.readthedocs.io/en/stable/README.html</a></p>
<p>感觉主要是在几个数据集上用各种工具集成的命令和注解文档。</p>
<p>数据集有：</p>
<ol>
<li><p>English Gigaword v5</p>
<p>Gigaword语料库包含45亿字，它们来自于1994-2010年1000万篇英文新闻通讯文章。</p>
<p>来源：Parker and Robert et al. English Gigaword Fifth Edition LDC2011T07. Web Download, Philadelphia: Linguistic Data Consortium, 2011.</p>
</li>
<li><p>Annotated NYT（纽约时报注释语料库）</p>
<p>纽约时报注释语料库是1987-2007年《纽约时报》180万篇文章的集合。《纽约时报》的工作人员用近50种元数据（metadata）丰富了这些文章。这些元数据对不同类型的文件、发稿台和文章中提到的重要人物、地点和组织的列表进行分级分类。</p>
<p>来源：Evan Sandhaus. The New York Times Annotated Corpus LDC2008T19. Web Download, Philadelphia: Linguistic Data Consortium, 2008.</p>
</li>
<li><p>ColdStart</p>
<p>2014年的ColdStart国际知识图谱构建比赛是基于一个大约有5万个文档的语料库，其中大概70%来自Gigaword语料库，其余是非正式博客和论坛帖子的集合。这个集合由ColdStart组织者用BBN的Serif工具进行预处理，然后发布的。</p>
</li>
<li><p>Wikipedia </p>
<p>维基百科数据。</p>
</li>
</ol>
<p>注：语言数据协会（LDC）是由大学，图书馆，公司和政府研究实验室组成的开放性协会，旨在解决语言技术研究和开发中面临的关键数据短缺问题。</p>
<p>LDC上大部分数据都要收费，从数百美元到数千美元不等，但也有一些免费数据。无论需求数据是否免费，都需要在网站上注册并加入组织。</p>
<p>注册成功后，网站会提醒你的申请信息已经发给管理员。这时候只需要联系本校管理员帮你通过验证加入组织即可。</p>
<p>如果组织是会员，应该可以免费下载数据集。</p>
]]></content>
      <tags>
        <tag>调研报告</tag>
      </tags>
  </entry>
  <entry>
    <title>聊天机器人系统</title>
    <url>/2020/12/06/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<p>本文介绍了聊天机器人的研究背景、系统框架、实现方法（检索式聊天机器人和生成式聊天机器人）中有代表性的经典模型、两种实现方法的比较还有聊天机器人领域研究的挑战和展望</p>
<a id="more"></a>

<h2 id="基于多轮对话模型的聊天机器人系统"><a href="#基于多轮对话模型的聊天机器人系统" class="headerlink" title="基于多轮对话模型的聊天机器人系统"></a>基于多轮对话模型的聊天机器人系统</h2><h3 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h3><ol>
<li><p>聊天机器人研究背景</p>
</li>
<li><p>聊天机器人系统框架</p>
</li>
<li><p>聊天机器人的实现方法</p>
<ul>
<li>检索式聊天机器人</li>
<li>生成式聊天机器人</li>
<li>两种方法的比较</li>
</ul>
</li>
<li><p>聊天机器人研究存在的挑战</p>
</li>
<li><p>聊天机器人在研究上的展望</p>
</li>
</ol>
<hr>
<h3 id="聊天机器人研究背景"><a href="#聊天机器人研究背景" class="headerlink" title="聊天机器人研究背景"></a>聊天机器人研究背景</h3><p>首先，我们先通过一个问题来引入聊天机器人——“机器会思考吗？”。这是1950年图灵在《Mind》上发表的《计算机器和智能（Computing Machinery and Intelligence）》一文中提出的问题。为了验证“机器”能否“思考”，他又提出了一个“模仿游戏”，后来被称为“图灵测试”，该测试的依据是衡量一台机器的智能，在多大程度上与人类的智能相像，或不可区分。</p>
<p>测试的具体内容是让计算机程序虚拟真人与询问者进行即时对话，询问者只能根据对话的内容，去判断谈话的对象是机器还是真人；如果询问者不能做到这一点，该机器就通过图灵测试了。</p>
<p><img src="/2020/12/06/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%B3%BB%E7%BB%9F/%E5%9B%BE%E7%81%B5%E6%B5%8B%E8%AF%95.png" alt="图灵测试"></p>
<p>可以看出，聊天机器人是图灵测试的一种实现方式，而图灵测试又被认为是人工智能的终极目标，这极大地激发了人们对聊天机器人的研究兴趣。</p>
<p>另一方面，近年来，各种聊天机器人应运而生。从应用场景的角度来看，可以分为在线客服、娱乐、教育、个人助理和智能问答五个种类。 </p>
<ul>
<li>在线客服聊天机器人系统的主要功能是同用户进行基本沟通并自动回复用户有关产品或服务的问题，以实现降低企业客服运营成本、提升用户体验的目的。其应用场景通常为网站首页和手机终端。代表性的商用系统有小I机器人、京东的JIMI客服机器人等。</li>
<li>娱乐场景下聊天机器人系统的主要功能是同用户进行开放主题的对话，从而实现对用户的精神陪伴、情感慰藉和心理疏导等作用。其应用场景通常为社交媒体、儿童玩具等。代表性的系统如微软“小冰”、微信“小微”、“小黄鸡”等。</li>
<li>教育场景下的聊天机器人主要功能包括帮助用户学习某种语言；在学习某项专业技能中，指导用户学习、掌握某项专业技能；辅助用户在特定年龄阶段学习某种知识等。其应用场景通常为具备人机交互功能的学习、培训类软件以及智能玩具等。比如科大讯飞公司的开心熊宝智能玩具，它可以通过语音对话的形式辅助儿童学习唐诗、宋词以及回答简单的常识性问题。 </li>
<li>个人助理类应用主要通过语音或文字与聊天机器人系统进行交互，实现个人事务的查询及代办功能，如天气查询、短信收发、智能搜索等。其应用场景通常为便携式移动终端设备。代表性的商业系统有苹果Siri、百度度秘、微软小娜等。</li>
<li>智能问答类的聊天机器人主要功能是理解并回答用户提出的问题，这些问题比较侧重于事实性问题或者是需要计算和逻辑推理型的问题。代表性的系统有IBM Watson、Wolfram Alpha等。</li>
</ul>
<p>可以看到，聊天机器人也具有很好的民用价值，符合科研及产品化的发展方向。</p>
<hr>
<h3 id="聊天机器人系统框架"><a href="#聊天机器人系统框架" class="headerlink" title="聊天机器人系统框架"></a>聊天机器人系统框架</h3><p><img src="/2020/12/06/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%B3%BB%E7%BB%9F/%E6%A1%86%E6%9E%B6.png" alt="系统框架"></p>
<p>如图显示了一个通用的聊天机器人的系统框架，其中包含五个主要的功能模块（输入预处理模块、自然语言理解模块、对话管理模块、答案生成模块和输出处理模块）。基本的流程是：</p>
<ul>
<li>用户通过文字形式或者语音形式输入之后进行预处理，转化成文本形式进行自然语言理解；从完整的语句中提取出所需要的关键信息，并理解用户输入语句的含义，产生特定的语义表达式；然后进入对话管理模块，识别出用户输入语句是属于封闭式（要执行某种任务）还是开放式地进行闲聊，进而选择相应的对话模型；接着根据当前对话的模型进行答案的提取；最后将得到的答案转换为文本形式或者进行语音合成输出给用户。</li>
</ul>
<hr>
<h3 id="聊天机器人的实现方法"><a href="#聊天机器人的实现方法" class="headerlink" title="聊天机器人的实现方法"></a>聊天机器人的实现方法</h3><p> 实现对话系统的技术方法主要被划分为检索式模型与生成式模型。</p>
<ul>
<li><p>检索式对话模型的主要思想是根据用户输入的句子，在对话语料库中搜索匹配语义最相近的句子，将它们进行排序，从而得到最合适的应答内容，输出给用户。其工作流程图如下图所示：</p>
<p><img src="/2020/12/06/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%B3%BB%E7%BB%9F/%E6%A3%80%E7%B4%A2%E5%BC%8F%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E6%B5%81%E7%A8%8B%E5%9B%BE.png" alt="检索式对话系统流程图"></p>
<ul>
<li><p>检索式对话模型比较经典的代表技术是基于交互的多轮对话文本匹配模型SMN（Sequential Matching Network），其结构图如下：</p>
<p><img src="/2020/12/06/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%B3%BB%E7%BB%9F/SMN%E7%BB%93%E6%9E%84%E5%9B%BE.png" alt="SMN结构图"></p>
<ul>
<li>第一层是对话-回复匹配层。在这一层中首先对历史对话中的每条语句和候选回复进行词嵌入的处理得到相应的词向量表示。然后通过它们的词向量计算出词语级的相似度矩阵M1。</li>
<li>然后，将词嵌入的结果通过第一个GRU网络进行编码，得到历史对话和候选回复对应的隐藏状态序列。然后通过它们计算语句级的相似度矩阵M2。</li>
<li>将得到的词相似度矩阵M1和句相似度矩阵M2组成一个匹配信息张量，由于此张量是二维的，所以用CNN对其进行建模，通过卷积和池化操作对词语级和语句级的关系特征进行提取，得到关键信息的特征向量。</li>
<li>第二层是匹配积累层。将上一层中提取的特征向量输入到第二个GRU中，按照时间顺序对历史对话进行建模，得到隐藏状态序列。</li>
<li>第三层是匹配预测层。这一层通过第二层得到的隐藏状态序列计算侯选回复与历史对话的匹配程度，从而选择最合适的语句进行输出。</li>
</ul>
</li>
</ul>
</li>
<li><p>生成式对话模型的核心思想是根据用户输入的句子，利用模型逐词或逐字生成答案，然后将答案回复给用户。</p>
<ul>
<li><p>其中，多数技术都采用了Encoder-Decoder模型，即编码-解码模型，其框架技术原理如下图所示：</p>
<p><img src="/2020/12/06/%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%B3%BB%E7%BB%9F/%E7%BC%96%E7%A0%81%E8%A7%A3%E7%A0%81%E6%A8%A1%E5%9E%8B%E6%A1%86%E6%9E%B6%E5%9B%BE.png" alt="编码解码模型框架图"></p>
<ul>
<li>Encoder就是对输入序列进行编码，通过一系列非线性变换转化成一个带有语义的固定长度的向量；</li>
<li>Decoder就是根据之前生成的固定向量再转化成输出序列。</li>
<li>实现对话系统的时候，Encoder和Decoder都不是固定的，可选择的模型有RNN、LSTM、GRU等，可以自由组合。</li>
</ul>
</li>
</ul>
</li>
<li><p>两种方法的比较</p>
<table>
<thead>
<tr>
<th align="center">构建方法</th>
<th align="center">检索式</th>
<th align="center">生成式</th>
</tr>
</thead>
<tbody><tr>
<td align="center">聊天语料库</td>
<td align="center">问答句对</td>
<td align="center">问答句对</td>
</tr>
<tr>
<td align="center">人工工作量</td>
<td align="center">较大</td>
<td align="center">较低</td>
</tr>
<tr>
<td align="center">语法准确性</td>
<td align="center">准确</td>
<td align="center">不能保证</td>
</tr>
<tr>
<td align="center">应答可控性</td>
<td align="center">较高</td>
<td align="center">较低</td>
</tr>
<tr>
<td align="center">可扩展性</td>
<td align="center">一般</td>
<td align="center">较好</td>
</tr>
<tr>
<td align="center">技术难度</td>
<td align="center">中等</td>
<td align="center">较难</td>
</tr>
</tbody></table>
<p>（技术难度：生成式模型很难训练，回复时经常存在一些语法错误。）</p>
</li>
</ul>
<hr>
<h3 id="聊天机器人研究存在的挑战"><a href="#聊天机器人研究存在的挑战" class="headerlink" title="聊天机器人研究存在的挑战"></a>聊天机器人研究存在的挑战</h3><p>当前，对于聊天机器人的研究存在的挑战主要包括：</p>
<ul>
<li>对话上下文建模：聊天是一个有特定背景的连续交互过程，在这一过程中经常出现上下文省略和指代的情况。一句话的意义有时要结合对话上下文或者相关的背景才能确定，因此对于长对话文本上下文的建模是很重要的，也是一个难点。</li>
<li>对话过程中的知识表示：聊天机器人相关的领域任务可能有复杂的组成，牵涉很多的因素，只有了解这些因素的关系和含义，才能与用户做到真正意义上的交流。</li>
<li>聊天机器人智能程度的评价：目前，虽然可以采用一些通用的客观评价标准对聊天机器人进行评价，如回答正确率、任务完成率、对话回合数、对话时间等；但却无法衡量用户满意度、回复的连续性、一致性以及自然度。</li>
</ul>
<hr>
<h3 id="聊天机器人在研究上的展望"><a href="#聊天机器人在研究上的展望" class="headerlink" title="聊天机器人在研究上的展望"></a>聊天机器人在研究上的展望</h3><p>如果说传统的聊天机器人关注的是“智商”，即聊天机器人的信息和知识获取能力的话，那么今后的聊天机器人研究则更加注重“情商”，即聊天机器人的个性化情感抚慰、心理疏导和精神陪护等能力。 </p>
<ul>
<li><p>比如前几年创办的医疗类 AI 聊天机器人Woebot，主要用于缓解使用者的抑郁、焦虑症状。吴恩达也以董事长的身份加入了这家公司，他说“如果我们的机器能有一些真正地治疗师的洞察力和同情心，即使只是在网上聊天，也可以帮助到数百万人。”</p>
<p>可见这种“化疗”机器人，应该是未来聊天机器人发展的重要方向之一。</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>调研报告</tag>
      </tags>
  </entry>
  <entry>
    <title>西瓜书第三章思维导图</title>
    <url>/2020/11/22/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/</url>
    <content><![CDATA[<p>日常学习周志华教授的西瓜书，读完第三章：线性模型，绘制了思维导图，方便日后复习。</p>
<a id="more"></a>

<h1 id="线性模型思维导图"><a href="#线性模型思维导图" class="headerlink" title="线性模型思维导图"></a>线性模型思维导图</h1><p><img src="/2020/11/22/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B.png" alt="西瓜书第三章思维导图"></p>
<hr>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h3><ul>
<li>线性模型试图学得一个通过属性的线性组合来进行预测的函数</li>
</ul>
<h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ul>
<li>形式简单、易于建模</li>
<li>许多非线性模型可在线性模型的基础上通过引入层级结构或高维映射而得</li>
<li>线性模型中各个属性前的系数w直观地表达了各属性在预测中的重要性，因此线性模型有很好的可解释性</li>
</ul>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><h3 id="试图学得一个线性模型以尽可能准确地预测实值输出标记"><a href="#试图学得一个线性模型以尽可能准确地预测实值输出标记" class="headerlink" title="试图学得一个线性模型以尽可能准确地预测实值输出标记"></a>试图学得一个线性模型以尽可能准确地预测实值输出标记</h3><ul>
<li><p>若属性只有一个</p>
<ul>
<li><p>存在“序”关系</p>
<ul>
<li><p>二值属性</p>
<ul>
<li>用0/1表示两个属性值</li>
</ul>
</li>
<li><p>三值属性</p>
<ul>
<li>用0/0.5/1表示三个属性值</li>
</ul>
</li>
</ul>
</li>
<li><p>无“序”关系</p>
<ul>
<li><p>多个属性值用类似“独热向量”的方式表示，如(0,0,1)/(0,1,0)/(1,0,0)。</p>
<ul>
<li><p>注意</p>
<ul>
<li>如果将无序属性连续化，则会不恰当地引入序关系，对后续处理如距离计算等造成误差。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>如何衡量预测值与真实值之间的差别？</p>
<ul>
<li><p>均方误差</p>
<ul>
<li><p>均方误差的几何意义对应了欧几里得距离（欧氏距离）</p>
</li>
<li><p>基于均方误差最小化的模型求解方法是“最小二乘法”。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。</p>
<ul>
<li>寻找欧氏距离最小化的过程称为线性回归模型的最小二乘“参数估计”。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>若属性有多个</p>
<ul>
<li><p>多元线性回归</p>
<ul>
<li>使均方误差最小化的解不唯一，选择哪个解，将由学习算法的归纳偏好决定，常见的做法是引入正则化项。</li>
</ul>
</li>
</ul>
</li>
<li><p>特别地</p>
<ul>
<li><p>对数线性回归</p>
<ul>
<li>令模型预测值逼近y的对数，反应了所对应的输出标记在指数尺度上的变化。</li>
<li>形式上仍是线性回归，但实质上已是在求取输入空间到输出空间的非线性函数映射。</li>
</ul>
</li>
<li><p>广义线性模型</p>
<ul>
<li><p>将上述的对数函数换成任意单调可微函数（连续且充分），其中，这个函数称为“联系函数”。对数线性回归是广义线性模型在联系函数为对数函数时的特例。</p>
<ul>
<li><p>参数估计方法</p>
<ul>
<li>加权最小二乘法或极大似然法</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h2><h3 id="解决分类任务"><a href="#解决分类任务" class="headerlink" title="解决分类任务"></a>解决分类任务</h3><ul>
<li><p>二分类任务</p>
<ul>
<li><p>需要将线性回归模型产生的预测值（实值）转换为0/1</p>
<ul>
<li><p>单位阶跃函数</p>
<ul>
<li>若预测值大于0就判为正例</li>
<li>若预测值小于零就判为反例</li>
<li>若预测值为临界值零则可任意判别</li>
</ul>
</li>
<li><p>对数几率函数</p>
<ul>
<li><p>是一种Sigmoid函数</p>
<ul>
<li>将预测值转化为一个接近0或1的y值，并且其输出值在0附近变化地很陡。</li>
</ul>
</li>
<li><p>实际上是在用线性回归模型的预测结果去逼近真实标记的对数几率</p>
<ul>
<li><p>注意</p>
<ul>
<li>也有文献称为“逻辑回归”</li>
<li>虽然名字是“回归”，但却是一种分类学习方法</li>
</ul>
</li>
</ul>
</li>
<li><p>优点</p>
<ul>
<li>直接对分类的可能性进行建模，无需事先假设数据分布，避免了假设分布不准确所带来的问题</li>
<li>不仅预测出“类别”，而且可得到近似概率预测，这对许多需利用概率辅助决策的任务很有用</li>
<li>对数几率函数（简称对率函数）是任意阶可导的凸函数，有很好的数学性质，现有的许多数值优化算法都可直接用于求取最优解。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="线性判别分析"><a href="#线性判别分析" class="headerlink" title="线性判别分析"></a>线性判别分析</h2><h3 id="给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能地接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。"><a href="#给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能地接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。" class="headerlink" title="给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能地接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。"></a>给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能地接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别。</h3><ul>
<li><p>LDA（线性判别分析）的欲最大化目标是广义瑞利商</p>
<ul>
<li>二分类</li>
<li>可推广至多分类</li>
</ul>
</li>
</ul>
<h2 id="多分类学习"><a href="#多分类学习" class="headerlink" title="多分类学习"></a>多分类学习</h2><h3 id="有些二分类学习方法可推广至多分类"><a href="#有些二分类学习方法可推广至多分类" class="headerlink" title="有些二分类学习方法可推广至多分类"></a>有些二分类学习方法可推广至多分类</h3><ul>
<li>比如LDA的推广</li>
</ul>
<h3 id="更一般地，基于一些基本策略，利用二分类学习器来解决多分类问题"><a href="#更一般地，基于一些基本策略，利用二分类学习器来解决多分类问题" class="headerlink" title="更一般地，基于一些基本策略，利用二分类学习器来解决多分类问题"></a>更一般地，基于一些基本策略，利用二分类学习器来解决多分类问题</h3><ul>
<li><p>基本思路</p>
<ul>
<li><p>“拆解法”，即将多分类任务拆为若干个二分类任务求解。</p>
<ul>
<li><p>具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器；在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果。</p>
<ul>
<li><p>关键问题</p>
<ul>
<li>如何对多分类任务进行拆分？</li>
<li>如何对多个分类器进行集成？</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="拆分策略"><a href="#拆分策略" class="headerlink" title="拆分策略"></a>拆分策略</h3><ul>
<li><p>一对一（OvO）</p>
<ul>
<li>将N个类别两两配对，从而产生N(N-1)/2个二分类任务；在测试阶段，新样本将同时提交给所有分类器，于是我们将得到N(N-1)/2个分类结果，最终结果可通过投票（或各分类器的预测置信度等信息）产生。</li>
</ul>
</li>
<li><p>一对其余（OvR）</p>
<ul>
<li>每次将一个类的样例作为正例、所有其他类的样例作为反例来训练N个分类器。在测试时若只有一个分类器预测为正类，则对应的类别标记作为最终分类结果；否则，通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为分类结果。</li>
</ul>
</li>
<li><p>多对多（MvM）</p>
<ul>
<li><p>每次将若干个类作为正类，若干个其他类作为反类。</p>
<ul>
<li>一对一和一对其余均是多对多的特例</li>
</ul>
</li>
<li><p>MvM的正、反类构造必须有特殊的设计，不能随意选取。</p>
<ul>
<li><p>纠错输出码（ECOC）</p>
<ul>
<li><p>最常用的MvM技术。ECOC是将编码的思想引入类别拆分，并尽可能在解码过程中具有容错性。</p>
<ul>
<li><p>步骤一：编码：对N个类别做M次划分，每次划分将一部分类别划为正类，一部分划为反类，从而形成一个二分类训练集；这样一共产生M个训练集，可训练出M个分类器。</p>
<ul>
<li><p>类别划分通过“编码矩阵”指定。编码矩阵有多种形式，常见的主要有二元码、三元码。</p>
<ul>
<li>二元码：将每个类别分别指定为正类和反类。</li>
<li>三元码：在正、反类之外，还可指定“停用类”（不使用该类样本）。</li>
</ul>
</li>
</ul>
</li>
<li><p>步骤二：解码：M个分类器分别对测试样本进行预测，这些预测标记组成一个编码。将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果。</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="类别不平衡问题"><a href="#类别不平衡问题" class="headerlink" title="类别不平衡问题"></a>类别不平衡问题</h2><h3 id="当分类任务中不同类别的训练样例数目不平衡且差别较大时，如何进行学习？"><a href="#当分类任务中不同类别的训练样例数目不平衡且差别较大时，如何进行学习？" class="headerlink" title="当分类任务中不同类别的训练样例数目不平衡且差别较大时，如何进行学习？"></a>当分类任务中不同类别的训练样例数目不平衡且差别较大时，如何进行学习？</h3><ul>
<li><p>基本策略：再缩放（再平衡）rescaling</p>
<ul>
<li><p>y’/1-y’ = (y/1-y) * (m-/m+)</p>
<ul>
<li><p>再缩放的思想简单，但实际操作并不平凡，主要因为“训练集是真实样本总体的无偏采样”这个假设往往并不成立，也就是说，我们未必能有效地基于训练集观测几率来推断出真实几率。</p>
<ul>
<li><p>现有技术大体上有三类做法</p>
<ul>
<li>对训练集里的反类样例（这里假设反类样例远多于正类样例）进行“欠采样”（亦称“下采样”），即去除一些反例使得正、反例数目接近，然后再进行学习。</li>
<li>对训练集里的正类样例（这里假设反类样例远多于正类样例）进行“过采样”（亦称“上采样”），即增加一些正例使得正、反例数目接近，然后再进行学习。</li>
<li>基于原始训练集进行学习，但在用训练好的分类器进行预测时，将前面的式子嵌入到其决策过程中，称为“阈值移动”。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>值得一提，“再缩放”也是“代价敏感学习”的基础。</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>西瓜书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title>西瓜书第四章思维导图</title>
    <url>/2021/03/14/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC%E5%9B%9B%E7%AB%A0%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/</url>
    <content><![CDATA[<p>继续学习周志华教授的西瓜书，读完第四章：决策树，绘制了思维导图，方便日后复习。</p>
<a id="more"></a>

<h1 id="决策树思维导图"><a href="#决策树思维导图" class="headerlink" title="决策树思维导图"></a>决策树思维导图</h1><p><img src="/2021/03/14/%E8%A5%BF%E7%93%9C%E4%B9%A6%E7%AC%AC%E5%9B%9B%E7%AB%A0%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE/%E5%86%B3%E7%AD%96%E6%A0%91.png" alt="西瓜书第四章思维导图"></p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="决策树是一类常见的机器学习方法。"><a href="#决策树是一类常见的机器学习方法。" class="headerlink" title="决策树是一类常见的机器学习方法。"></a>决策树是一类常见的机器学习方法。</h3><ul>
<li>以二分类任务为例，我们希望从给定训练数据集学得一个模型用以对新示例进行分类，这个把样本分类的任务，可看作对“当前样本属于正类吗？”这个问题的“决策”或“判定”过程。</li>
<li>决策过程的最终结论对应了我们所希望的判定结果。</li>
<li>决策过程中提出的每个判定问题都是对某个属性的“测试”。</li>
<li>每个测试的结果或是导出最终结论，或是导出进一步的判定问题，其考虑范围是在上次决策结果的限定范围之内。</li>
</ul>
<h3 id="顾名思义，决策树是基于树结构来进行决策的。"><a href="#顾名思义，决策树是基于树结构来进行决策的。" class="headerlink" title="顾名思义，决策树是基于树结构来进行决策的。"></a>顾名思义，决策树是基于树结构来进行决策的。</h3><ul>
<li><p>一般的，一棵决策树包含一个根节点、若干个内部结点和若干个叶结点。</p>
<ul>
<li>叶结点对应于决策结果，其他每个结点则对应于一个属性测试；</li>
<li>根结点包含样本全集；</li>
<li>每个结点包含的样本集合根据属性测试的结果被划分到子结点中；</li>
<li>从根结点到每个叶结点的路径对应了一个判定测试序列。</li>
</ul>
</li>
</ul>
<h3 id="决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。"><a href="#决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。" class="headerlink" title="决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。"></a>决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。</h3><ul>
<li><p>基本流程</p>
<ul>
<li><p>遵循“分而治之”策略。</p>
</li>
<li><p>决策树的生成是一个递归过程。</p>
<ul>
<li><p>在决策树基本算法中，有三种情形会导致递归返回：</p>
<ul>
<li><p>当前结点包含的样本全属于同一类别，无需划分；</p>
<ul>
<li>标记当前结点为叶结点，并将其类别设定为该类别。</li>
</ul>
</li>
<li><p>当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；</p>
<ul>
<li><p>标记当前结点为叶结点，并将其类别设定为该结点所含样本最多的类别。</p>
</li>
<li><p>处理实质</p>
<ul>
<li>利用当前结点的后验分布</li>
</ul>
</li>
</ul>
</li>
<li><p>当前结点包含的样本集合为空，不能划分。</p>
<ul>
<li><p>标记当前结点为叶结点，但将其类别设定为其父结点所含样本数最多的类别。</p>
</li>
<li><p>处理实质</p>
<ul>
<li>把父结点的样本分布作为当前结点的先验分布。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="划分选择"><a href="#划分选择" class="headerlink" title="划分选择"></a>划分选择</h2><h3 id="决策树学习的关键是“如何选择最优划分属性”。"><a href="#决策树学习的关键是“如何选择最优划分属性”。" class="headerlink" title="决策树学习的关键是“如何选择最优划分属性”。"></a>决策树学习的关键是“如何选择最优划分属性”。</h3><ul>
<li><p>随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”越来越高。</p>
<ul>
<li><p>信息增益</p>
<ul>
<li><p>“信息熵”是度量样本集合纯度最常用的一种指标。</p>
<ul>
<li>信息熵的值越小，当前样本集合的纯度越高。信息熵最小值为0。</li>
<li>根据信息熵可以计算出用某种属性对样本集进行划分所获得的“信息增益”。</li>
<li>一般而言，信息增益越大，意味着使用该种属性来进行划分所获得的“纯度提升”越大。</li>
</ul>
</li>
</ul>
</li>
<li><p>增益率</p>
<ul>
<li>为解决信息增益准则偏好可能带来的不利影响所提出</li>
<li>著名的C4.5决策树算法使用“增益率”来选择最优划分属性。</li>
</ul>
</li>
<li><p>基尼指数</p>
<ul>
<li><p>CART决策树使用“基尼指数”来选择划分属性，CART可用于分类和回归任务。</p>
</li>
<li><p>数据集的纯度可用基尼值来度量。</p>
<ul>
<li>基尼值反映了从数据集中随机抽取两个样本，其类别标记不一致的概率。</li>
<li>基尼指数越小，则数据集的纯度越高</li>
</ul>
</li>
<li><p>基尼指数在基尼值的基础上引入了权重计算</p>
<ul>
<li>因此，我们在侯选属性集合中，选择那个使得划分后基尼指数最小的属性作为最优划分属性。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h2><h3 id="剪枝是决策树学习算法对付“过拟合”的主要手段。"><a href="#剪枝是决策树学习算法对付“过拟合”的主要手段。" class="headerlink" title="剪枝是决策树学习算法对付“过拟合”的主要手段。"></a>剪枝是决策树学习算法对付“过拟合”的主要手段。</h3><ul>
<li><p>决策树分支过多，可能使训练样本学的“太好”了，把一些样本自身的特点当作所有数据都具有的一般性质而导致过拟合。因此可通过主动去掉一些分支来降低过拟合的风险。</p>
<ul>
<li><p>预剪枝</p>
<ul>
<li><p>预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点。</p>
<ul>
<li><p>采用2.2节介绍的性能评估方法。划分数据集为训练集和验证集，训练集用于标记节点类别，验证集用于估计泛化能力。</p>
<ul>
<li><p>结论</p>
<ul>
<li><p>只要不能提升验证集精度（降低或不改变）都会禁止节点划分。</p>
</li>
<li><p>预剪枝使决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。</p>
</li>
<li><p>但有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高。</p>
<ul>
<li>预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>后剪枝</p>
<ul>
<li><p>后剪枝是先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行考察，若该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。</p>
<ul>
<li><p>采用2.2节介绍的性能评估方法。划分数据集为训练集和验证集，训练集用于标记节点类别，验证集用于估计泛化能力。</p>
<ul>
<li><p>结论</p>
<ul>
<li><p>当剪枝使验证集精度提高时，进行剪枝</p>
</li>
<li><p>当剪枝使验证集精度降低时，不能剪枝</p>
</li>
<li><p>当剪枝使验证集精度不变时，可以剪枝也可以不剪枝</p>
<ul>
<li>但根据奥卡姆剃刀准则，剪枝后的模型更好。因此，实际决策树算法在这种情况下通常要进行剪枝。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="连续与缺失值"><a href="#连续与缺失值" class="headerlink" title="连续与缺失值"></a>连续与缺失值</h2><h3 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h3><ul>
<li><p>在决策树学习中使用连续属性</p>
<ul>
<li><p>问题</p>
<ul>
<li>连续属性的可取值数目不再有限，因此，不能直接根据连续属性的可取值来对节点进行划分。</li>
</ul>
</li>
<li><p>解决</p>
<ul>
<li><p>连续属性离散化技术</p>
<ul>
<li><p>最简单的策略是采用二分法对连续属性进行处理，这是C4.5决策树算法中采用的机制。</p>
<ul>
<li>把连续区间的中位点作为候选划分点。按照这个点t进行二分，不大于t的样本记作D-，大于t的样本记作D+。然后可像离散属性值一样来考察这些划分点，选取最优的划分点进行样本集合的划分。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>注意</p>
<ul>
<li>与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><ul>
<li><p>现实任务中常会遇到不完整样本，即样本的某些属性值缺失。如果简单地放弃不完整样本，仅使用无缺失值的样本来进行学习，显然是对数据信息极大的浪费。因此，有必要考虑利用有缺失属性值的训练样例来进行学习。</p>
<ul>
<li><p>待解决的问题</p>
<ul>
<li><p>如何在属性值缺失的情况下进行划分属性选择？</p>
<ul>
<li>可根据样本集中在该属性上没有缺失值的样本子集来判断该属性的优劣。</li>
</ul>
</li>
<li><p>给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？</p>
<ul>
<li>让同一个样本以不同的概率划入到不同的子结点中。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="多变量决策树"><a href="#多变量决策树" class="headerlink" title="多变量决策树"></a>多变量决策树</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><ul>
<li>若把每个属性视为坐标空间中的一个坐标轴，则d个属性描述的样本就对应了d维空间中的一个数据点，对样本分类意味着在这个坐标空间中寻找不同类样本之间的分类边界。</li>
<li>决策树所形成的分类边界有一个明显特点：轴平行，即它的分类边界由若干个与坐标轴平行的分段组成。</li>
<li>分类边界的每一段都是与坐标轴平行的，这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个属性取值。</li>
<li>但在学习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似。此时的决策树会相当复杂，由于要进习大量的属性测试，预测时间开销会很大。</li>
<li>若能使用斜的划分边界，则决策树模型将大为简化。</li>
</ul>
<h3 id="多变量决策树就是能实现这样的“斜划分”甚至更复杂划分的决策树。因此也称“斜决策树”。"><a href="#多变量决策树就是能实现这样的“斜划分”甚至更复杂划分的决策树。因此也称“斜决策树”。" class="headerlink" title="多变量决策树就是能实现这样的“斜划分”甚至更复杂划分的决策树。因此也称“斜决策树”。"></a>多变量决策树就是能实现这样的“斜划分”甚至更复杂划分的决策树。因此也称“斜决策树”。</h3><ul>
<li><p>定义</p>
<ul>
<li>在多变量决策树中，非叶节点不再是仅对某个属性，而是对属性的线性组合进行测试。</li>
<li>与传统的“单变量决策树”不同，在多变量决策树的学习过程中，不是为每个非叶节点寻找一个最优划分属性，而是试图建立一个合适的线性分类器。</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>西瓜书笔记</tag>
      </tags>
  </entry>
</search>
